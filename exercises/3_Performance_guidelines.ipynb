{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"3_Performance_guidelines.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"outer-promise"},"source":["# Lecture 3. Performance guidelines."],"id":"outer-promise"},{"cell_type":"markdown","metadata":{"id":"J7GiIP54uVnu"},"source":["In this notebook we will describe the most important guidelines when programming code for NVIDIA GPU cards.\n","\n","We will consider the following aspects of optimizing CUDA kernels:\n","- memory access patterns: *memory coalescing* for the best throughput,\n","- control flow: how code branching affects the performance,\n","- multiprocessor occupancy: experimenting with different block sizes,\n","- instruction-level optimizations: avoiding data type conversion, using floating point *intrisincs*."],"id":"J7GiIP54uVnu"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iavk9Cs4i_Ic","executionInfo":{"status":"ok","timestamp":1624793206766,"user_tz":-120,"elapsed":1721,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"abc29480-3c64-4cdf-c5c7-263ca27ac732"},"source":["! pip install --upgrade --force-reinstall git+https://github.com/pjarosik/ius-2021-gpu-short-course.git"],"id":"Iavk9Cs4i_Ic","execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/pjarosik/ius-2021-gpu-short-course.git\r\n","  Cloning https://github.com/pjarosik/ius-2021-gpu-short-course.git to /tmp/pip-req-build-hqfc5k4j\r\n","  Running command git clone -q https://github.com/pjarosik/ius-2021-gpu-short-course.git /tmp/pip-req-build-hqfc5k4j\n","Building wheels for collected packages: gpu-short-course\n","  Building wheel for gpu-short-course (setup.py) ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Created wheel for gpu-short-course: filename=gpu_short_course-0.0.1-py3-none-any.whl size=3119 sha256=34d57b30f61b6bfbb52418e76cdacc7ef9a58b3cae3d8dc1aea6fcd82fbc58c7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-gus5xq4n/wheels/4f/07/fc/9537d8ac1b84ce9cde4db4fcebd10fd77e93ea2c18fcb8a656\n","Successfully built gpu-short-course\n","Installing collected packages: gpu-short-course\n","  Attempting uninstall: gpu-short-course\n","    Found existing installation: gpu-short-course 0.0.1\n","    Uninstalling gpu-short-course-0.0.1:\n","      Successfully uninstalled gpu-short-course-0.0.1\n","Successfully installed gpu-short-course-0.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K7TzNH4sUfFd"},"source":["## Exercise 3.1. Memory access patterns."],"id":"K7TzNH4sUfFd"},{"cell_type":"markdown","metadata":{"id":"Q2u5NmF997ls"},"source":["According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.\n","\n","This means that the number of useful memory accesses done by our kernel, and thus its performance, largely depends on the memory access pattern it does.\n","\n","Our goal is to implement a GPU kernel that only loads **useful** data from global memory that will then be used in the calculations. We can achieve this with **coalesced memory accesses**.\n","\n","To achive coalesced memory accesses in our kernel, we need to meet the following conditions:\n","- the number of threads per block is a multiple of 32 threads,\n","- sequential threads in a warp access memory that is sequential.\n","\n","\n","For example, our baseline `add_vectors_gpu` and `convolve_gpu` implementations satisfy the above conditions:\n","- the number of threads per block was equal 256,\n","- adjacent threads were reading adjacent memory areas, e.g. thread `i` read the `a[i]` and `b[i]`, and thread `i+1` read `a[i+1]` and `b[i+1]`.  \n"],"id":"Q2u5NmF997ls"},{"cell_type":"markdown","metadata":{"id":"hVK75cukFY1P"},"source":["We will discuss below what are the reasons for both of the conditions."],"id":"hVK75cukFY1P"},{"cell_type":"markdown","metadata":{"id":"pn3fT7ViAlSV"},"source":["### Exercise 3.1.1. Impact of misaligned accesses."],"id":"pn3fT7ViAlSV"},{"cell_type":"markdown","metadata":{"id":"7g2ZEr-MERds"},"source":["According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing."],"id":"7g2ZEr-MERds"},{"cell_type":"markdown","metadata":{"id":"qmYr7eNmEpn7"},"source":["Recall that warp reads global memory by using a sequence of **32-byte** segments transactions.\n","\n","Note that, according to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> Memory allocated through the CUDA Runtime API (...) is guaranteed to be aligned to at least 256 bytes.\n","\n","This means that if the block size is a multiple of the warp size, each block will load only its own data chunk from memory. \n"],"id":"qmYr7eNmEpn7"},{"cell_type":"markdown","metadata":{"id":"2CgD_Nl-ONbm"},"source":["#### Example\n","\n","As an example, we will consider here:\n","- `add_vectors_gpu` function,\n","- block size = 13. \n","\n","Now let's take a look what memory accesses will be performed by thread block 0, 1, etc.\n","\n","**Block 0**\n","\n","- reads memory area [0, 52), size: 52 bytes (13 x 4-byte floats)\n","\n","```\n","[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n","[       block 0 data (52 bytes)       ]\n","```\n","\n","- This requires 2 x 32-byte transfers. \n","- However only 52 bytes will be used (81%). \n","\n","**Block 1**\n","\n","- Reads memory area [52, 104), size: 52 bytes.\n","```\n","[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n","                                       [       block 1 data (52 bytes)       ]\n","```\n","- This requires 3 x 32-byte transfers.\n","- However only 52 bytes will be used (54%). \n","\n","\n","**And so on...**\n","\n","\n","As we can see, we are transfer a large amount of (theoretically) useless data."],"id":"2CgD_Nl-ONbm"},{"cell_type":"markdown","metadata":{"id":"moOL-8cYJT12"},"source":["Let's see if there is any observable performance difference between scripts using 256 and 261 threads in a **block**:"],"id":"moOL-8cYJT12"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrPeRyL4hIRu","executionInfo":{"status":"ok","timestamp":1624793206802,"user_tz":-120,"elapsed":31,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"faf4d8fe-e24e-4248-b416-0dc718771bb0"},"source":["%%writefile 3_1_1_aligned.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 256\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"qrPeRyL4hIRu","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_1_1_aligned.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rkdpd78-iBQ6","executionInfo":{"status":"ok","timestamp":1624793210846,"user_tz":-120,"elapsed":4045,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"5348af40-01ae-4be3-c544-34c6394e274c"},"source":["! nvprof --trace gpu python 3_1_1_aligned.py"],"id":"rkdpd78-iBQ6","execution_count":null,"outputs":[{"output_type":"stream","text":["==120747== NVPROF is profiling process 120747, command: python 3_1_1_aligned.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0136 seconds (+/- 0.0338), median: 0.0091\n","==120747== Profiling application: python 3_1_1_aligned.py\n","==120747== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   55.47%  411.13ms       300  1.3704ms  1.2867ms  2.6277ms  [CUDA memcpy DtoH]\n","                   41.17%  305.19ms       200  1.5259ms  1.3844ms  2.9887ms  [CUDA memcpy HtoD]\n","                    3.36%  24.899ms       100  248.99us  247.87us  267.11us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZNAOdG8h-xB","executionInfo":{"status":"ok","timestamp":1624793210895,"user_tz":-120,"elapsed":14,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"5f0980cd-902e-4d75-fecb-45f8af3e07d1"},"source":["%%writefile 3_1_1_misaligned.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 261\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"yZNAOdG8h-xB","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_1_1_misaligned.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5TINGpYldmo","executionInfo":{"status":"ok","timestamp":1624793214953,"user_tz":-120,"elapsed":4057,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"7f5cb1d0-006d-44a5-a63d-78d75caab994"},"source":["! nvprof --trace gpu python 3_1_1_misaligned.py"],"id":"G5TINGpYldmo","execution_count":null,"outputs":[{"output_type":"stream","text":["==120777== NVPROF is profiling process 120777, command: python 3_1_1_misaligned.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0135 seconds (+/- 0.0362), median: 0.0092\n","==120777== Profiling application: python 3_1_1_misaligned.py\n","==120777== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   55.17%  403.34ms       300  1.3445ms  1.2913ms  2.8496ms  [CUDA memcpy DtoH]\n","                   41.37%  302.47ms       200  1.5123ms  1.3749ms  2.1925ms  [CUDA memcpy HtoD]\n","                    3.46%  25.261ms       100  252.61us  250.91us  275.04us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kdy0Y21Tlvij"},"source":["The difference in performance of the aligned and misaligned versions is rather minor (on some devices even negligible).\n","\n","Why? \n","\n","In this particular case, adjacent warps **reuse the cached data** their neighbors fetched. \n","\n","Anyway, setting a block size to a multiple of warp size, might be a good rule of thumb: it facilitates coalescing, and (as we will discuss later), helps to avoid wasting multiprocessor computation time on under-populated warps."],"id":"Kdy0Y21Tlvij"},{"cell_type":"markdown","metadata":{"id":"WVH-4tO1ApIv"},"source":["### Exercise 3.1.2. Impact of strided accesses."],"id":"WVH-4tO1ApIv"},{"cell_type":"markdown","metadata":{"id":"QAD5qaCi8yhV"},"source":["A non-unit-strided global memory accesses may impact effective memory bandwidth. \n","\n","We say that GPU kernel performs a unit-strided memory access, if threads with successive identifiers read the data from successive memory areas, in other words, the following access pattern is respected:\n","\n","```\n","x = data[(some custom offset) + threadIdx.x]\n","```\n","\n","When using the data access notation for a multidimensional array, make sure that the last axis is addressed using `threadIdx.x`:\n","\n","```\n","x = data[(other dimensions...), threadIdx.x]\n","```\n","\n","\n","The degradation in performance can be especially apparent when working with multidimensional arrays - the choice of the axis, along which choosing a specific operation is performed, can affect effective bandwidth."],"id":"QAD5qaCi8yhV"},{"cell_type":"markdown","metadata":{"id":"urpbVefb9WPH"},"source":["#### Example"],"id":"urpbVefb9WPH"},{"cell_type":"markdown","metadata":{"id":"driving-activation"},"source":["Let's consider a 1D convolution along one of the axes of a 2D array.\n","\n","```\n","         axis 1\n","     ---------------\n","x = [[0,  1,  2,  3], |\n","     [4,  5,  6,  7], | axis 0\n","     [8,  9, 10, 11]] |\n","\n","h = [1, 1]\n","\n","```\n","\n","NumPy stores arrays in row-major order, so the above array is actually kept in computer's memory as a following 1D array:\n","\n","```\n","x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] \n","```"],"id":"driving-activation"},{"cell_type":"markdown","metadata":{"id":"possible-packaging"},"source":["Let's consider doing convolution along axis 0 and 1.\n","\n","**Convolve along axis 1**:\n","\n","```\n","         axis 1\n","     ---------------\n","x = [[0,  1,  2,  3]  *  [1, 1] = [0,  1,  3,  5] \n","     [4,  5,  6,  7], *  [1, 1] = [4,  9, 11, 13]\n","     [8,  9, 10, 11]] *  [1, 1] = [8, 17, 19, 21]\n","```\n","\n","For the first output row:\n","\n","```\n","x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n","    [1]      y = [0]\n","    [1, 1]       [1]\n","       [1, 1]    [3]\n","          [1, 1] [5]\n","\n","```\n","\n","`y[0]` `y[1]`, `y[2]` and `y[3]` are computed by threads with `threadIdx.x` equal `0`, `1`, `2` and `3`, respectively.\n","\n","**Convolve along axis 0**:\n","\n","```\n","x = [[0,  1,   2,   3]  | \n","     [4,  5,   6,   7], | axis 0\n","     [8,  9,  10,  11]] | \n","      *   *    *    *\n","     [1] [1]  [1]  [1]\n","     [1] [1]  [1]  [1]\n","   y  =   =    =    =\n","    [ 0] [ 1] [ 2] [ 3]\n","    [ 4] [ 6] [ 8] [10]\n","    [12] [14] [16] [18]\n","```\n","\n","For the first output column:\n","\n","```\n","x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n","    [1]                                    y = [ 0]\n","    [1,          1]                            [ 4]\n","                [1,         1]                 [12]\n","```\n","\n","`y[0]` `y[1]`, and `y[2]` are computed by threads with `threadIdx.x` equal `0`, `1` and `2` respectively."],"id":"possible-packaging"},{"cell_type":"markdown","metadata":{"id":"qsmm48sLaOFB"},"source":["As we can see in the above example, the stride is much larger for the convolution along axis 0. Will it impact the bandwidth?"],"id":"qsmm48sLaOFB"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adequate-experiment","executionInfo":{"status":"ok","timestamp":1624793215022,"user_tz":-120,"elapsed":66,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"b77422d7-92af-4e7a-bd05-b2288af55e15"},"source":["%%writefile 3_1_2_convolve_strided_access.py\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","import cupy as cp\n","import gpu_short_course\n","\n","@cuda.jit\n","def convolve_axis0_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n","\n","    N = len(h)\n","    o = int(math.ceil(N/2)-1)\n","    HEIGHT = x.shape[0]\n","    WIDTH = x.shape[1]\n","    if i >= HEIGHT or j >= WIDTH:\n","        return\n","    \n","    value = float32(0.0)\n","    for k in range(N):\n","        l = i + o - k\n","        if l >= 0 and l < HEIGHT:\n","\n","            ## --- Get data along the second (1) axis.\n","            value += x[l, j] * h[k]\n","            \n","    y[i, j] = value\n","    \n","    \n","def convolve_axis0_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block = (32, 32)\n","    height, width = x.shape\n","    block_h, block_w = block\n","    grid = (math.ceil(width/block_w), \n","            math.ceil(height/block_h))\n","    convolve_axis0_gpu_kernel[grid, block](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","@cuda.jit\n","def convolve_axis1_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n","\n","    N = len(h)\n","    o = int(math.ceil(N/2)-1)\n","    \n","    HEIGHT = x.shape[0]\n","    WIDTH = x.shape[1]\n","    \n","    if i >= WIDTH or j >= HEIGHT:\n","        return\n","    \n","    value = float32(0.0)\n","    for k in range(N):\n","        l = i + o - k\n","        if l >= 0 and l < WIDTH:\n","            \n","            ## --- Get data along the first (0) axis.\n","            value += x[j, l]*h[k]\n","            \n","    y[j, i] = value\n","    \n","    \n","def convolve_axis1_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block = (32, 32)\n","    height, width = x.shape\n","    block_h, block_w = block\n","    grid = (math.ceil(width/block_w), \n","            math.ceil(height/block_h))\n","    convolve_axis1_gpu_kernel[grid, block](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","gpu_short_course.run_convolve_2d_input(convolve_axis0_gpu, axis=0)\n","gpu_short_course.run_convolve_2d_input(convolve_axis1_gpu, axis=1)"],"id":"adequate-experiment","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_1_2_convolve_strided_access.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rising-tuning","executionInfo":{"status":"ok","timestamp":1624793216189,"user_tz":-120,"elapsed":1166,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"4c74105d-4aab-418c-a095-a948386b1fa7"},"source":["! python 3_1_2_convolve_strided_access.py --mode test"],"id":"rising-tuning","execution_count":null,"outputs":[{"output_type":"stream","text":["GPU:0: b'GeForce MX250'\n","All tests passed.\n","All tests passed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"31NCkiJAy0OJ","executionInfo":{"status":"ok","timestamp":1624793230308,"user_tz":-120,"elapsed":14117,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"d1eb0750-a9f8-47f4-96c1-627475843530"},"source":["! nvprof --trace gpu python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1"],"id":"31NCkiJAy0OJ","execution_count":null,"outputs":[{"output_type":"stream","text":["==120832== NVPROF is profiling process 120832, command: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n","GPU:0: b'GeForce MX250'\n","Benchmarking, please wait...\n","Benchmarking, please wait...\n","==120832== Profiling application: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n","==120832== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   60.57%  6.42933s       100  64.293ms  50.388ms  97.451ms  cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   31.24%  3.31607s       100  33.161ms  25.843ms  46.395ms  cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                    5.27%  559.35ms       600  932.25us     768ns  2.9319ms  [CUDA memcpy DtoH]\n","                    2.92%  310.09ms       400  775.23us     928ns  3.1401ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CTgHQiOX1-vG"},"source":["On my GPU (Nvidia GeForce MX250), convolution along axis 1 takes much less time than along axis 0.\n","\n","We can use profiler metrics to verify what memory access efficiency for both cases we have:"],"id":"CTgHQiOX1-vG"},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"civilian-constant","executionInfo":{"status":"ok","timestamp":1624793236614,"user_tz":-120,"elapsed":6289,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"31917e38-75cb-4083-a866-1e82a46a3cdf"},"source":["! nvprof --metrics gld_efficiency,gst_efficiency python 3_1_2_convolve_strided_access.py --mode benchmark n=1 quiet=1 2>&1 | grep -v \"^=\""],"id":"civilian-constant","execution_count":null,"outputs":[{"output_type":"stream","text":["GPU:0: b'GeForce MX250'\r\n","Benchmarking, please wait...\r\n","Benchmarking, please wait...\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","          1                            gld_efficiency             Global Memory Load Efficiency      12.50%      12.50%      12.50%\n","          1                            gst_efficiency            Global Memory Store Efficiency      12.24%      12.24%      12.24%\n","    Kernel: cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","          1                            gld_efficiency             Global Memory Load Efficiency      70.05%      70.05%      70.05%\n","          1                            gst_efficiency            Global Memory Store Efficiency     100.00%     100.00%     100.00%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_2mBwUDf87yB"},"source":["# Exercise 3.2. Control flow: how code branching affects the performance."],"id":"_2mBwUDf87yB"},{"cell_type":"markdown","metadata":{"id":"5jF4RobZ2nDg"},"source":["Due to SIMT architecture of the CUDA multiprocessors, it is recommended to avoid different paths within the same warp.\n","\n","According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> Flow control instructions (if, switch, do, for, while) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp."],"id":"5jF4RobZ2nDg"},{"cell_type":"markdown","metadata":{"id":"nZNBa4eG2_Rj"},"source":["### Example"],"id":"nZNBa4eG2_Rj"},{"cell_type":"markdown","metadata":{"id":"4DwABDbg3BLR"},"source":["Let's implement the following function:\n","\n","```\n","y[i] = r[i]*a[i] + b[i]\n","```\n","\n","where `r[i] = i mod 8`.\n","\n","We can implement it in one of the two ways:\n","1. directly by definition (see `add_vectors_mod8_kernel`),\n","2. by doing a sequence of `if ... elif ... elif ... else` blocks (see `add_vectors_mod8_branches_kernel`)."],"id":"4DwABDbg3BLR"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEV3cwOb3r8m","executionInfo":{"status":"ok","timestamp":1624793236640,"user_tz":-120,"elapsed":7,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"9198f154-42ce-4341-9d9e-610ebdd0d8c2"},"source":["%%writefile 3_2_control_flow.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","\n","block_size = 256\n","\n","\n","@cuda.jit\n","def add_vectors_mod8_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    r = i % 8 + 1\n","    result[i] = r*a[i] + b[i]\n","\n","\n","def add_vectors_mod8(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_mod8_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","@cuda.jit\n","def add_vectors_mod8_branches_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    if i % 8 == 0:\n","        result[i] = a[i] + b[i]\n","    elif i % 8 == 1:\n","        result[i] = 2*a[i] + b[i]\n","    elif i % 8 == 2:\n","        result[i] = 3*a[i] + b[i]\n","    elif i % 8 == 3:\n","        result[i] = 4*a[i] + b[i]\n","    elif i % 8 == 4:\n","        result[i] = 5*a[i] + b[i]\n","    elif i % 8 == 5:\n","        result[i] = 6*a[i] + b[i]\n","    elif i % 8 == 6:\n","        result[i] = 7*a[i] + b[i]\n","    elif i % 8 == 7:\n","        result[i] = 8*a[i] + b[i]\n","\n","\n","def add_vectors_mod8_branches(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_mod8_branches_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8)\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8_branches)"],"id":"gEV3cwOb3r8m","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_2_control_flow.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yag_hEM8CEuc"},"source":["Let's check how much time does it take to execute each of the kernel:"],"id":"yag_hEM8CEuc"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3syxy_Dq4Piu","executionInfo":{"status":"ok","timestamp":1624793244268,"user_tz":-120,"elapsed":7627,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"649beb56-370d-442f-dfcf-a2aeb1061421"},"source":["! nvprof --trace gpu python 3_2_control_flow.py"],"id":"3syxy_Dq4Piu","execution_count":null,"outputs":[{"output_type":"stream","text":["==120889== NVPROF is profiling process 120889, command: python 3_2_control_flow.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0150 seconds (+/- 0.0370), median: 0.0106\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0168 seconds (+/- 0.0178), median: 0.0122\n","==120889== Profiling application: python 3_2_control_flow.py\n","==120889== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   41.58%  837.90ms       600  1.3965ms  1.2846ms  2.8637ms  [CUDA memcpy DtoH]\n","                   31.36%  631.89ms       400  1.5797ms  1.3816ms  3.3680ms  [CUDA memcpy HtoD]\n","                   22.81%  459.63ms       100  4.5963ms  1.4817ms  38.123ms  cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                    4.25%  85.578ms       100  855.78us  307.94us  4.7233ms  cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a-Tf_qxCBm9h"},"source":["Let's also measure `branch_efficiency` metric defined as a:\n","> Ratio of non-divergent branches to total branches expressed as percentage."],"id":"a-Tf_qxCBm9h"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7htXLWzAkEM","executionInfo":{"status":"ok","timestamp":1624793252171,"user_tz":-120,"elapsed":7883,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"6bc275a6-ee41-4a07-ed77-59318a7134b4"},"source":["! nvprof --metrics branch_efficiency python 3_2_control_flow.py"],"id":"v7htXLWzAkEM","execution_count":null,"outputs":[{"output_type":"stream","text":["==120924== NVPROF is profiling process 120924, command: python 3_2_control_flow.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0190 seconds (+/- 0.0341), median: 0.0144\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0193 seconds (+/- 0.0165), median: 0.0146\n","==120924== Profiling application: python 3_2_control_flow.py\n","==120924== Profiling result:\n","==120924== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                         branch_efficiency                         Branch Efficiency     100.00%     100.00%     100.00%\n","    Kernel: cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                         branch_efficiency                         Branch Efficiency      58.82%      58.82%      58.82%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7rq1MbBJI0CO"},"source":["Of course, the above example has been artificially complicated just to show the effect of complex kernel logic on the kernel's performance."],"id":"7rq1MbBJI0CO"},{"cell_type":"markdown","metadata":{"id":"DaX4hxR09G0B"},"source":["# Exercise 3.3. Multiprocessor occupancy: thread block size."],"id":"DaX4hxR09G0B"},{"cell_type":"markdown","metadata":{"id":"pIGIxvK6JG5n"},"source":["Recall that:\n","> The number of threads per block should be a **multiple of 32 threads**, because this provides optimal computing efficiency and facilitates coalescing.\n","\n","[CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) also gives some other suggestions how to choose the proper number of threads per block:\n","\n","> There are many such factors involved in selecting block size, and inevitably some experimentation is required. However, a few rules of thumb should be followed:\n","> 1. Threads per block should be **a multiple of warp size** to avoid wasting computation on under-populated warps and to facilitate coalescing.\n","> 2. A **minimum of 64 threads** per block should be used, and only if there are multiple concurrent blocks per multiprocessor.\n","> 3. Between **128 and 256 threads** per block is a good initial range for experimentation with different block sizes.\n","> 4. Use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. This is particularly beneficial to kernels that frequently call __syncthreads()."],"id":"pIGIxvK6JG5n"},{"cell_type":"markdown","metadata":{"id":"VToK2NYJRRgk"},"source":["### Example"],"id":"VToK2NYJRRgk"},{"cell_type":"markdown","metadata":{"id":"xq06puf9Q2md"},"source":["Let's mesure `add_vectors`' occupancy for a different number of threads:\n"],"id":"xq06puf9Q2md"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oho6SbZORUFs","executionInfo":{"status":"ok","timestamp":1624793252195,"user_tz":-120,"elapsed":7,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"7d26a1bd-05f7-4a21-ebea-ecf4d37d84dd"},"source":["%%writefile 3_3_occupancy_16.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 16\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"oho6SbZORUFs","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_3_occupancy_16.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J58oLJpDRi9k","executionInfo":{"status":"ok","timestamp":1624793256631,"user_tz":-120,"elapsed":4435,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"9a56b460-21aa-4469-be5f-be53df6240ae"},"source":["! nvprof --trace gpu python 3_3_occupancy_16.py"],"id":"J58oLJpDRi9k","execution_count":null,"outputs":[{"output_type":"stream","text":["==120971== NVPROF is profiling process 120971, command: python 3_3_occupancy_16.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0163 seconds (+/- 0.0370), median: 0.0108\n","==120971== Profiling application: python 3_3_occupancy_16.py\n","==120971== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   43.63%  416.23ms       300  1.3874ms  1.2930ms  2.6358ms  [CUDA memcpy DtoH]\n","                   31.96%  304.87ms       200  1.5243ms  1.4158ms  2.7896ms  [CUDA memcpy HtoD]\n","                   24.42%  232.96ms       100  2.3295ms  642.53us  16.229ms  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LKyfKBGpWCtR"},"source":["According to NVIDIA documentation, `achieved_occupancy` measures:\n","> Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor."],"id":"LKyfKBGpWCtR"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"725eQmA6V5Fj","executionInfo":{"status":"ok","timestamp":1624793261927,"user_tz":-120,"elapsed":5279,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"60ddba76-8a60-472f-be6a-2fbeab3c7939"},"source":["! nvprof --metrics achieved_occupancy python 3_3_occupancy_16.py"],"id":"725eQmA6V5Fj","execution_count":null,"outputs":[{"output_type":"stream","text":["==120999== NVPROF is profiling process 120999, command: python 3_3_occupancy_16.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0236 seconds (+/- 0.0420), median: 0.0174\n","==120999== Profiling application: python 3_3_occupancy_16.py\n","==120999== Profiling result:\n","==120999== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                        achieved_occupancy                        Achieved Occupancy    0.302620    0.336189    0.322717\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBy_xzl6Rd0m","executionInfo":{"status":"ok","timestamp":1624793261954,"user_tz":-120,"elapsed":7,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"a70eaa06-142d-40a7-85a9-ece016f88db0"},"source":["%%writefile 3_3_occupancy_256.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 256\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"WBy_xzl6Rd0m","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_3_occupancy_256.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYxXfEU8WOWU","executionInfo":{"status":"ok","timestamp":1624793270081,"user_tz":-120,"elapsed":8126,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"c7fa80d2-8ed7-471b-a341-3f870249399c"},"source":["! nvprof --trace gpu python 3_3_occupancy_256.py\n","! nvprof --metrics achieved_occupancy python 3_3_occupancy_256.py"],"id":"EYxXfEU8WOWU","execution_count":null,"outputs":[{"output_type":"stream","text":["==121026== NVPROF is profiling process 121026, command: python 3_3_occupancy_256.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0126 seconds (+/- 0.0315), median: 0.0090\n","==121026== Profiling application: python 3_3_occupancy_256.py\n","==121026== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   54.84%  392.64ms       300  1.3088ms  1.2852ms  1.5636ms  [CUDA memcpy DtoH]\n","                   41.69%  298.46ms       200  1.4923ms  1.3814ms  2.6135ms  [CUDA memcpy HtoD]\n","                    3.47%  24.828ms       100  248.28us  247.39us  257.15us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n","==121054== NVPROF is profiling process 121054, command: python 3_3_occupancy_256.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0168 seconds (+/- 0.0278), median: 0.0130\n","==121054== Profiling application: python 3_3_occupancy_256.py\n","==121054== Profiling result:\n","==121054== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                        achieved_occupancy                        Achieved Occupancy    0.808703    0.898269    0.869669\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFUXkREwReV8","executionInfo":{"status":"ok","timestamp":1624793270106,"user_tz":-120,"elapsed":9,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"b882475c-0cb8-456b-e7b9-4b09d089dfc2"},"source":["%%writefile 3_3_occupancy_1024.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 1024\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"TFUXkREwReV8","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_3_occupancy_1024.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNGVqycVWXyB","executionInfo":{"status":"ok","timestamp":1624793278575,"user_tz":-120,"elapsed":8468,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"267592da-17a9-449d-fde0-2c8f03ac3910"},"source":["! nvprof --trace gpu python 3_3_occupancy_1024.py\n","! nvprof --metrics achieved_occupancy python 3_3_occupancy_1024.py"],"id":"CNGVqycVWXyB","execution_count":null,"outputs":[{"output_type":"stream","text":["==121085== NVPROF is profiling process 121085, command: python 3_3_occupancy_1024.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0143 seconds (+/- 0.0316), median: 0.0105\n","==121085== Profiling application: python 3_3_occupancy_1024.py\n","==121085== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   53.86%  422.55ms       300  1.4085ms  1.2888ms  2.6570ms  [CUDA memcpy DtoH]\n","                   40.29%  316.07ms       200  1.5804ms  1.3943ms  3.2166ms  [CUDA memcpy HtoD]\n","                    5.86%  45.945ms       100  459.45us  249.19us  1.1231ms  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n","==121120== NVPROF is profiling process 121120, command: python 3_3_occupancy_1024.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0160 seconds (+/- 0.0307), median: 0.0126\n","==121120== Profiling application: python 3_3_occupancy_1024.py\n","==121120== Profiling result:\n","==121120== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                        achieved_occupancy                        Achieved Occupancy    0.729894    0.833186    0.791537\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1dAsTyfuWmC0"},"source":["Finding the right number of threads per block requires some experimentation, but generally 128 or 256 threads is a good starting point."],"id":"1dAsTyfuWmC0"},{"cell_type":"markdown","metadata":{"id":"Ufa_O1_J9vTI"},"source":["# Exercise 3.4. Instruction-level optimizations."],"id":"Ufa_O1_J9vTI"},{"cell_type":"markdown","metadata":{"id":"GX352UzFpPsX"},"source":["This chapter covers the following:\n","\n","1. Impact of the data type selection and conversion on the kernel's performance.\n","2. Floating-point intrinsics."],"id":"GX352UzFpPsX"},{"cell_type":"markdown","metadata":{"id":"Cn_-MQH1Vn4l"},"source":["## Exercise 3.4.1. Data types."],"id":"Cn_-MQH1Vn4l"},{"cell_type":"markdown","metadata":{"id":"DUaJzNKup3SX"},"source":["When implementing a CUDA GPU kernel, keep the following in mind:\n","\n","- the choice of the input data type may affect the kernel performance,\n","- data type conversion in kernel implementation may affect kernel performance."],"id":"DUaJzNKup3SX"},{"cell_type":"markdown","metadata":{"id":"oRKBDy19qKZI"},"source":["Let's do some comparison of  `float32` and `float64` data, based on an example of a one-dimensional convolution.\n","\n","First, let's recall our baseline implementation of convolution operator.\n","\n","NOTE:\n","- our benchmark function generates `float32` input data,\n","- we used in the kernel implementation the `float32` keyword to enforce the proper data type of `value` variable. "],"id":"oRKBDy19qKZI"},{"cell_type":"code","metadata":{"id":"2Mq2y1_fW48S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624793278601,"user_tz":-120,"elapsed":7,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"6f695fdd-f94a-432c-d31f-03ba34bc614e"},"source":["%%writefile 3_4_1_convolve_float32.py\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","from gpu_short_course.tests import benchmark_convolve\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","    M, N = len(x), len(h)\n","    \n","    o = int(math.ceil(N/2)-1)\n","\n","    value = float32(0.0)\n","    for j in range(N):\n","        k = i+o-j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","    \n","\n","def convolve_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","benchmark_convolve(convolve_gpu, dtype=np.float32)"],"id":"2Mq2y1_fW48S","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_4_1_convolve_float32.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6iwhZPo4W8OM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624793292682,"user_tz":-120,"elapsed":14080,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"d0db9dda-91a2-4dd1-ea20-70c9afdd2e88"},"source":["! nvprof --trace gpu python 3_4_1_convolve_float32.py"],"id":"6iwhZPo4W8OM","execution_count":null,"outputs":[{"output_type":"stream","text":["==121149== NVPROF is profiling process 121149, command: python 3_4_1_convolve_float32.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.1231 seconds (+/- 0.0903), median: 0.0957\n","==121149== Profiling application: python 3_4_1_convolve_float32.py\n","==121149== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   96.46%  11.3062s       100  113.06ms  19.506ms  348.87ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                    2.26%  265.12ms       300  883.73us  1.1840us  2.6277ms  [CUDA memcpy DtoH]\n","                    1.27%  149.27ms       200  746.35us  1.0240us  1.6264ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kj_TCkJVsHfP"},"source":["Now let's get rid of the `float32` keyword on line `17` and see if it has any effect on performance."],"id":"kj_TCkJVsHfP"},{"cell_type":"code","metadata":{"id":"Wp7G2hWfW-5y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624793292711,"user_tz":-120,"elapsed":8,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"7cd57113-c7dc-4f8a-e48e-fa30ce71fde0"},"source":["%%writefile 3_4_1_convolve_float32_and_float64.py\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","from gpu_short_course.tests import benchmark_convolve\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","    M, N = len(x), len(h)\n","    \n","    o = int(math.ceil(N/2)-1)\n","\n","    value = 0.0\n","    for j in range(N):\n","        k = i+o-j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","    \n","\n","def convolve_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","benchmark_convolve(convolve_gpu, dtype=np.float32)"],"id":"Wp7G2hWfW-5y","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_4_1_convolve_float32_and_float64.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3_ezLMZXASn","executionInfo":{"status":"ok","timestamp":1624793308268,"user_tz":-120,"elapsed":15556,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"57c6c826-a43b-4944-c9e4-f11c2bfd880a"},"source":["! nvprof --trace gpu python 3_4_1_convolve_float32_and_float64.py"],"id":"q3_ezLMZXASn","execution_count":null,"outputs":[{"output_type":"stream","text":["==121192== NVPROF is profiling process 121192, command: python 3_4_1_convolve_float32_and_float64.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.1373 seconds (+/- 0.1268), median: 0.0682\n","==121192== Profiling application: python 3_4_1_convolve_float32_and_float64.py\n","==121192== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   96.84%  12.7187s       100  127.19ms  30.170ms  532.16ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                    2.03%  266.36ms       300  887.85us  1.2160us  2.6352ms  [CUDA memcpy DtoH]\n","                    1.13%  148.28ms       200  741.41us     960ns  2.5882ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z6c7gjfpXC9v"},"source":["The processing may increase, because 0.0 is a float64, and we are doing promotion from float32 to float64, i.e.:\n","\n","`value += (float64)(x[k]*h[j])`\n","\n","then we downgrade from float64 to float32:\n","\n","`y[i] = (float32)value`.\n","\n","(note: the above may vary between different GPUs)"],"id":"Z6c7gjfpXC9v"},{"cell_type":"markdown","metadata":{"id":"sUntEbVrXG6W"},"source":["Lets check what results we will get when we will use only `float64` values in computations."],"id":"sUntEbVrXG6W"},{"cell_type":"code","metadata":{"id":"N7jeCX3vXIk3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624793308295,"user_tz":-120,"elapsed":6,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"e5d5408f-8938-4e00-c462-05dddd52719b"},"source":["%%writefile 3_4_1_convolve_float64.py\n","\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","from gpu_short_course.tests import benchmark_convolve\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","    M, N = len(x), len(h)\n","    \n","    o = int(math.ceil(N/2)-1)\n","\n","    value = 0.0\n","    for j in range(N):\n","        k = i+o-j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","    \n","\n","def convolve_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","benchmark_convolve(convolve_gpu, dtype=np.float64)"],"id":"N7jeCX3vXIk3","execution_count":null,"outputs":[{"output_type":"stream","text":["Writing 3_4_1_convolve_float64.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UnnVJp5JXKHg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624793327703,"user_tz":-120,"elapsed":19407,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"4b15e567-8a6a-44b5-daf8-b24fe7d93496"},"source":["! nvprof --trace gpu python 3_4_1_convolve_float64.py"],"id":"UnnVJp5JXKHg","execution_count":null,"outputs":[{"output_type":"stream","text":["==121243== NVPROF is profiling process 121243, command: python 3_4_1_convolve_float64.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.1755 seconds (+/- 0.0391), median: 0.1882\n","==121243== Profiling application: python 3_4_1_convolve_float64.py\n","==121243== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   94.90%  16.0141s       100  160.14ms  20.212ms  245.10ms  cudapy::__main__::convolve_gpu_kernel$241(Array<double, int=1, C, mutable, aligned>, Array<double, int=1, C, mutable, aligned>, Array<double, int=1, C, mutable, aligned>)\n","                    3.24%  547.14ms       300  1.8238ms  1.3120us  5.1162ms  [CUDA memcpy DtoH]\n","                    1.86%  313.50ms       200  1.5675ms  1.1840us  6.1230ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"484XwMSnugNk"},"source":["The above results may differ between different GPU cards."],"id":"484XwMSnugNk"},{"cell_type":"markdown","metadata":{"id":"eRPoAi2G9yay"},"source":["## Exercise 3.4.2. Floating point intrisincs."],"id":"eRPoAi2G9yay"},{"cell_type":"markdown","metadata":{"id":"A4kVCX0lurgR"},"source":["CUDA Math API provides a set of intrinsic functions, specialized to carry out various calculations. Sometimes, using an intrisinc function explicitly in your code may improve its performance.\n","\n","A complete list of intrinsic functions for CUDA C/C++ is available [here](https://docs.nvidia.com/cuda/cuda-math-api/).\n","\n","A complete list of floating-point intrinsics in Numba is avalable [here](https://numba.pydata.org/numba-doc/latest/cuda-reference/kernel.html#floating-point-intrinsics). \n","\n","Note: At the stage of optimizing the machine code, the compiler may decide to use the intrinsic function (the same or similar), regardless of whether we used it in our implementation or not. Still, if you want to be sure that the intrinsic function is used, we should call it explicitly."],"id":"A4kVCX0lurgR"},{"cell_type":"markdown","metadata":{"id":"kl2i6H0cwI41"},"source":["Let's check if using `cuda.fma` intrinsic in our `convolution` gives us any improvement:"],"id":"kl2i6H0cwI41"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Z5QSCr8XopT","executionInfo":{"status":"ok","timestamp":1624793374573,"user_tz":-120,"elapsed":20,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"40cb571b-f9d4-47e6-da10-fe62ac289d35"},"source":["%%writefile 3_4_2_convolve_intrinsics.py\n","import math\n","from numba import cuda, float32\n","import numpy as np\n","import gpu_short_course.tests\n","import cupy as cp\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","\n","    M, N = len(x), len(h)\n","    o = int(math.ceil(N/2)-1)\n","    \n","    value = float32(0.0)\n","    for j in range(N):\n","        k = i + o - j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","\n","\n","def convolve(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","@cuda.jit\n","def convolve_fma_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","\n","    M, N = len(x), len(h)\n","    o = int(math.ceil(N/2)-1)\n","    \n","    value = float32(0.0)\n","    for j in range(N):\n","        k = i + o - j\n","        if k >= 0 and k < M:\n","            value = cuda.fma(x[k], h[j], value)\n","    y[i] = value\n","\n","\n","def convolve_fma(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_fma_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_convolve(convolve_fma)\n","gpu_short_course.tests.benchmark_convolve(convolve)"],"id":"0Z5QSCr8XopT","execution_count":null,"outputs":[{"output_type":"stream","text":["Overwriting 3_4_2_convolve_intrinsics.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMjAyrbSY9gP","executionInfo":{"status":"ok","timestamp":1624793385776,"user_tz":-120,"elapsed":8993,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"27684aaa-21b9-4c07-b20c-5eaae86da32f"},"source":["!nvprof --trace gpu python 3_4_2_convolve_intrinsics.py"],"id":"JMjAyrbSY9gP","execution_count":null,"outputs":[{"output_type":"stream","text":["==121341== NVPROF is profiling process 121341, command: python 3_4_2_convolve_intrinsics.py\n","GPU:0: b'GeForce MX250'\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0323 seconds (+/- 0.0499), median: 0.0270\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0322 seconds (+/- 0.0107), median: 0.0313\n","==121341== Profiling application: python 3_4_2_convolve_intrinsics.py\n","==121341== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   44.91%  2.38789s       100  23.879ms  18.964ms  30.330ms  cudapy::__main__::convolve_gpu_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   37.76%  2.00778s       100  20.078ms  17.149ms  25.120ms  cudapy::__main__::convolve_fma_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   11.46%  609.14ms       600  1.0152ms     928ns  2.6273ms  [CUDA memcpy DtoH]\n","                    5.87%  311.97ms       400  779.93us     928ns  2.5739ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]}]}