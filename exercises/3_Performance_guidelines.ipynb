{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"3_Performance_guidelines.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"outer-promise"},"source":["# Lecture 3. Performance guidelines."],"id":"outer-promise"},{"cell_type":"markdown","metadata":{"id":"J7GiIP54uVnu"},"source":["In this notebook we will the most important guidelines when programming code for NVIDIA GPU cards.\n","\n","We will consider the following aspects of optimizing CUDA kernels:\n","- memory access patterns: *memory coalescing* for the best throughput,\n","- control flow: how code branching affects the performance,\n","- multiprocessor occupancy: experimenting with different block sizes,\n","- instruction-level optimizations: avoiding data type conversion, using floating point *intrisincs*."],"id":"J7GiIP54uVnu"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Iavk9Cs4i_Ic","executionInfo":{"status":"ok","timestamp":1624685279475,"user_tz":-120,"elapsed":1832,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"0a648530-84e6-410f-d54c-df1ee83b5eee"},"source":["! pip install --upgrade --force-reinstall git+https://github.com/pjarosik/ius-2021-gpu-short-course.git"],"id":"Iavk9Cs4i_Ic","execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/pjarosik/ius-2021-gpu-short-course.git\r\n","  Cloning https://github.com/pjarosik/ius-2021-gpu-short-course.git to /tmp/pip-req-build-pytg2ac3\r\n","  Running command git clone -q https://github.com/pjarosik/ius-2021-gpu-short-course.git /tmp/pip-req-build-pytg2ac3\n","Building wheels for collected packages: gpu-short-course\n","  Building wheel for gpu-short-course (setup.py) ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Created wheel for gpu-short-course: filename=gpu_short_course-0.0.1-py3-none-any.whl size=2924 sha256=9c78b40d750dc3ddd1d47d13ac10f34157d417e85ac8aba3820cb4dcb4dc7821\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-okxpyrfb/wheels/4f/07/fc/9537d8ac1b84ce9cde4db4fcebd10fd77e93ea2c18fcb8a656\n","Successfully built gpu-short-course\n","Installing collected packages: gpu-short-course\n","  Attempting uninstall: gpu-short-course\n","    Found existing installation: gpu-short-course 0.0.1\n","    Uninstalling gpu-short-course-0.0.1:\n","      Successfully uninstalled gpu-short-course-0.0.1\n","Successfully installed gpu-short-course-0.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"K7TzNH4sUfFd"},"source":["## Exercise 3.1. Memory access patterns."],"id":"K7TzNH4sUfFd"},{"cell_type":"markdown","metadata":{"id":"Q2u5NmF997ls"},"source":["According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.\n","\n","This means that the number of useful memory accesses done by our kernel, and thus its performance, largely depends on the memory access pattern it does.\n","\n","Our goal is to implement a GPU kernel that only loads **useful** data from global memory that will then be used in the calculations. We can achieve this with **coalesced memory accesses**.\n","\n","To achive coalesced memory accesses in our kernel, we need to meet the following conditions:\n","- the number of threads per block is a multiple of 32 threads,\n","- sequential threads in a warp access memory that is sequential.\n","\n","\n","For example, our baseline `add_vectors_gpu` and `convolve_gpu` implementations satisfy the above conditions:\n","- the number of threads per block was equal 256,\n","- adjacent threads were reading adjacent memory areas, e.g. thread `i` read the `a[i]` and `b[i]`, and thread `i+1` read `a[i+1]` and `b[i+1]`.  \n"],"id":"Q2u5NmF997ls"},{"cell_type":"markdown","metadata":{"id":"hVK75cukFY1P"},"source":["We will discuss below what are the reasons for both of the conditions."],"id":"hVK75cukFY1P"},{"cell_type":"markdown","metadata":{"id":"pn3fT7ViAlSV"},"source":["### Exercise 3.1.1. Impact of misaligned accesses."],"id":"pn3fT7ViAlSV"},{"cell_type":"markdown","metadata":{"id":"7g2ZEr-MERds"},"source":["According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing."],"id":"7g2ZEr-MERds"},{"cell_type":"markdown","metadata":{"id":"qmYr7eNmEpn7"},"source":["Recall that warp reads global memory by using a sequence **32-byte** segments transactions.\n","\n","Note that, according to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> Memory allocated through the CUDA Runtime API (...) is guaranteed to be aligned to at least 256 bytes.\n","\n","This means that if the block size is a multiple of the warp size, each block will load only its own data chunk from memory. \n"],"id":"qmYr7eNmEpn7"},{"cell_type":"markdown","metadata":{"id":"2CgD_Nl-ONbm"},"source":["#### Example\n","\n","As an example, we will consider here:\n","- `add_vectors_gpu` function,\n","- block size = 13. \n","\n","Now let's take a look what memory accesses will be performed by thread block 0, 1, etc.\n","\n","**Block 0**\n","\n","- reads memory area [0, 52), size: 52 bytes (13 x 4-byte floats)\n","\n","```\n","[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n","[       block 0 data (52 bytes)       ]\n","```\n","\n","- This requires 2 x 32-byte transfers. \n","- However only 52 bytes will be used (81%). \n","\n","**Block 1**\n","\n","- Reads memory area [52, 104), size: 52 bytes.\n","```\n","[ Segment 0 (32 bytes) ][ Segment 1 (32 bytes) ][ Segment 2 (32 bytes) ] ...\n","                                       [       block 1 data (52 bytes)       ]\n","```\n","- This requires 3 x 32-byte transfers.\n","- However only 52 bytes will be used (54%). \n","\n","\n","**And so on...**\n","\n","\n","As we can see, we are transfer (theoretically) a large amount of unnecessary data."],"id":"2CgD_Nl-ONbm"},{"cell_type":"markdown","metadata":{"id":"moOL-8cYJT12"},"source":["Let's see if there is any observable performance difference between scripts using 256 and 261 threads in a **block**:"],"id":"moOL-8cYJT12"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrPeRyL4hIRu","executionInfo":{"status":"ok","timestamp":1624637219458,"user_tz":-120,"elapsed":29,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"24e208bc-1758-4ee1-da9e-f85b21a657ed"},"source":["%%writefile 3_1_1_aligned.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 256\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"qrPeRyL4hIRu","execution_count":5,"outputs":[{"output_type":"stream","text":["Overwriting 3_1_1_aligned.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rkdpd78-iBQ6","executionInfo":{"status":"ok","timestamp":1624637363339,"user_tz":-120,"elapsed":8342,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"874e2967-2e9d-46c0-91b4-71f5a9703d98"},"source":["! nvprof --trace gpu python 3_1_1_aligned.py"],"id":"rkdpd78-iBQ6","execution_count":16,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==16609== NVPROF is profiling process 16609, command: python 3_1_1_aligned.py\n","Benchmark result: \n","Average processing time: 0.0486 seconds (+/- 0.3719), median: 0.0111\n","==16609== Profiling application: python 3_1_1_aligned.py\n","==16609== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   55.89%  419.12ms       300  1.3971ms  1.2828ms  2.8090ms  [CUDA memcpy DtoH]\n","                   40.77%  305.75ms       200  1.5287ms  1.3798ms  3.2402ms  [CUDA memcpy HtoD]\n","                    3.33%  24.986ms       100  249.86us  247.83us  271.22us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZNAOdG8h-xB","executionInfo":{"status":"ok","timestamp":1624637313964,"user_tz":-120,"elapsed":19,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"3a9dd92a-3f1d-4721-81f1-99607d18bb05"},"source":["%%writefile 3_1_1_misaligned.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 261\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"yZNAOdG8h-xB","execution_count":11,"outputs":[{"output_type":"stream","text":["Overwriting 3_1_1_misaligned.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5TINGpYldmo","executionInfo":{"status":"ok","timestamp":1624637341749,"user_tz":-120,"elapsed":5381,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"a3c3a148-0030-465e-b73e-6674ff5be954"},"source":["! nvprof --trace gpu python 3_1_1_misaligned.py"],"id":"G5TINGpYldmo","execution_count":14,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==16528== NVPROF is profiling process 16528, command: python 3_1_1_misaligned.py\n","Benchmark result: \n","Average processing time: 0.0192 seconds (+/- 0.0729), median: 0.0118\n","==16528== Profiling application: python 3_1_1_misaligned.py\n","==16528== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   57.59%  444.93ms       300  1.4831ms  1.2941ms  2.6090ms  [CUDA memcpy DtoH]\n","                   39.11%  302.13ms       200  1.5107ms  1.3879ms  2.5283ms  [CUDA memcpy HtoD]\n","                    3.30%  25.455ms       100  254.55us  250.71us  273.11us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Kdy0Y21Tlvij"},"source":["The difference in performance of the aligned and misaligned versions is rather minor (on some devices even negligible).\n","\n","Why? \n","\n","In this particular case, adjacent warps **reuse the cached data** their neighbors fetched. \n","\n","Anyway, setting a block size to a multiple of warp size, might be a good rule of thumb: it facilitates coalescing, and (as we will discuss later), helps to avoid wasting multiprocessor computation time on under-populated warps."],"id":"Kdy0Y21Tlvij"},{"cell_type":"markdown","metadata":{"id":"WVH-4tO1ApIv"},"source":["### Exercise 3.1.2. Impact of strided accesses."],"id":"WVH-4tO1ApIv"},{"cell_type":"markdown","metadata":{"id":"QAD5qaCi8yhV"},"source":["A non-unit-strided global memory accesses may impact effective memory bandwidth. \n","\n","We say that GPU kernel performs a unit-strided memory access, if threads with successive identifiers read the data from successive memory areas, in other words, the following access pattern is respected:\n","\n","```\n","x = data[(some custom offset) + threadIdx.x]\n","```\n","\n","When using the data access notation for a multidimensional array, make sure that the last axis is addressed using `threadIdx.x`:\n","\n","```\n","x = data[(other dimensions...), threadIdx.x]\n","```\n","\n","\n","The degradation in performance can be especially apparent when working with multidimensional arrays - the choice of the axis, along which choosing a specific operation is performed, can affect effective bandwidth."],"id":"QAD5qaCi8yhV"},{"cell_type":"markdown","metadata":{"id":"urpbVefb9WPH"},"source":["#### Example"],"id":"urpbVefb9WPH"},{"cell_type":"markdown","metadata":{"id":"driving-activation"},"source":["Let's consider a 1D convolution along one of the axes of a 2D array.\n","\n","```\n","         axis 1\n","     ---------------\n","x = [[0,  1,  2,  3], |\n","     [4,  5,  6,  7], | axis 0\n","     [8,  9, 10, 11]] |\n","\n","h = [1, 1]\n","\n","```\n","\n","NumPy stores arrays in row-major order, so the above array is actually kept in computer's memory as a following 1D array:\n","\n","```\n","x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11] \n","```"],"id":"driving-activation"},{"cell_type":"markdown","metadata":{"id":"possible-packaging"},"source":["Let's consider doing convolution along axis 0 and 1.\n","\n","**Convolve along axis 1**:\n","\n","```\n","         axis 1\n","     ---------------\n","x = [[0,  1,  2,  3]  *  [1, 1] = [0,  1,  3,  5] \n","     [4,  5,  6,  7], *  [1, 1] = [4,  9, 11, 13]\n","     [8,  9, 10, 11]] *  [1, 1] = [8, 17, 19, 21]\n","```\n","\n","For the first output row:\n","\n","```\n","x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n","    [1]      y = [0]\n","    [1, 1]       [1]\n","       [1, 1]    [3]\n","          [1, 1] [5]\n","\n","```\n","\n","`y[0]` `y[1]`, `y[2]` and `y[3]` are computed by threads with `threadIdx.x` equal `0`, `1`, `2` and `3`, respectively.\n","\n","**Convolve along axis 0**:\n","\n","```\n","x = [[0,  1,   2,   3]  | \n","     [4,  5,   6,   7], | axis 0\n","     [8,  9,  10,  11]] | \n","      *   *    *    *\n","     [1] [1]  [1]  [1]\n","     [1] [1]  [1]  [1]\n","   y  =   =    =    =\n","    [ 0] [ 1] [ 2] [ 3]\n","    [ 4] [ 6] [ 8] [10]\n","    [12] [14] [16] [18]\n","```\n","\n","For the first output column:\n","\n","```\n","x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]  \n","    [1]                                    y = [ 0]\n","    [1,          1]                            [ 4]\n","                [1,         1]                 [12]\n","```\n","\n","`y[0]` `y[1]`, and `y[2]` are computed by threads with `threadIdx.x` equal `0`, `1` and `2` respectively."],"id":"possible-packaging"},{"cell_type":"markdown","metadata":{"id":"qsmm48sLaOFB"},"source":["As we can see in the above example, the stride is much larger for the convolution along axis 0. Will it impact the bandwidth?"],"id":"qsmm48sLaOFB"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"adequate-experiment","executionInfo":{"status":"ok","timestamp":1624685343054,"user_tz":-120,"elapsed":25,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"7be751cd-9209-493d-ff42-f3fafb386235"},"source":["%%writefile 3_1_2_convolve_strided_access.py\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","import cupy as cp\n","import gpu_short_course\n","\n","@cuda.jit\n","def convolve_axis0_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n","\n","    N = len(h)\n","    o = int(math.ceil(N/2)-1)\n","    HEIGHT = x.shape[0]\n","    WIDTH = x.shape[1]\n","    if i >= HEIGHT or j >= WIDTH:\n","        return\n","    \n","    value = float32(0.0)\n","    for k in range(N):\n","        l = i + o - k\n","        if l >= 0 and l < HEIGHT:\n","\n","            ## --- Get data along the second (1) axis.\n","            value += x[l, j] * h[k]\n","            \n","    y[i, j] = value\n","    \n","    \n","def convolve_axis0_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block = (32, 32)\n","    height, width = x.shape\n","    block_h, block_w = block\n","    grid = (math.ceil(width/block_w), \n","            math.ceil(height/block_h))\n","    convolve_axis0_gpu_kernel[grid, block](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","@cuda.jit\n","def convolve_axis1_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    j = cuda.blockIdx.y*cuda.blockDim.y + cuda.threadIdx.y\n","\n","    N = len(h)\n","    o = int(math.ceil(N/2)-1)\n","    \n","    HEIGHT = x.shape[0]\n","    WIDTH = x.shape[1]\n","    \n","    if i >= WIDTH or j >= HEIGHT:\n","        return\n","    \n","    value = float32(0.0)\n","    for k in range(N):\n","        l = i+o-k\n","        if l >= 0 and l < WIDTH:\n","            \n","            ## --- Get data along the first (0) axis.\n","            value += x[j, l]*h[k]\n","            \n","    y[j, i] = value\n","    \n","    \n","def convolve_axis1_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block = (32, 32)\n","    height, width = x.shape\n","    block_h, block_w = block\n","    grid = (math.ceil(width/block_w), \n","            math.ceil(height/block_h))\n","    convolve_axis1_gpu_kernel[grid, block](y, x, h)\n","    return y.copy_to_host()\n","\n","gpu_short_course.convolve_2d_input(convolve_axis0_gpu, axis=0)\n","gpu_short_course.convolve_2d_input(convolve_axis1_gpu, axis=1)"],"id":"adequate-experiment","execution_count":2,"outputs":[{"output_type":"stream","text":["Overwriting 3_1_2_convolve_strided_access.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rising-tuning","executionInfo":{"status":"ok","timestamp":1624685349096,"user_tz":-120,"elapsed":1304,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"bd3aa8d9-830c-414d-b21b-fa5f05aee9a9"},"source":["! python 3_1_2_convolve_strided_access.py --mode test"],"id":"rising-tuning","execution_count":3,"outputs":[{"output_type":"stream","text":["All tests passed.\n","All tests passed.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"31NCkiJAy0OJ","executionInfo":{"status":"ok","timestamp":1624685449491,"user_tz":-120,"elapsed":13994,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"8e352f55-8f92-4af6-c904-a50395774bf4"},"source":["! nvprof --trace gpu python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1"],"id":"31NCkiJAy0OJ","execution_count":8,"outputs":[{"output_type":"stream","text":["Benchmarking, please wait...\n","==61799== NVPROF is profiling process 61799, command: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n","Benchmarking, please wait...\n","==61799== Profiling application: python 3_1_2_convolve_strided_access.py --mode benchmark quiet=1\n","==61799== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   59.41%  6.13775s       100  61.378ms  53.455ms  78.364ms  cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   31.94%  3.29992s       100  32.999ms  26.750ms  39.088ms  cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                    5.70%  588.47ms       600  980.79us     800ns  2.6401ms  [CUDA memcpy DtoH]\n","                    2.95%  304.83ms       400  762.08us     896ns  2.5595ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CTgHQiOX1-vG"},"source":["On my GPU (Nvidia GeForce MX250), convolution along axis 1 takes much less time than along axis 0.\n","\n","We can use profiler metrics to verify what memory access efficiency for both cases we have:"],"id":"CTgHQiOX1-vG"},{"cell_type":"code","metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"civilian-constant","executionInfo":{"status":"ok","timestamp":1624686095533,"user_tz":-120,"elapsed":4810,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"9762e1cb-79c0-4df2-dcc6-01ec77e25489"},"source":["! nvprof --metrics gld_efficiency,gst_efficiency python 3_1_2_convolve_strided_access.py --mode benchmark n=1 quiet=1 2>&1 | grep -v \"^=\""],"id":"civilian-constant","execution_count":11,"outputs":[{"output_type":"stream","text":["Benchmarking, please wait...\r\n","Benchmarking, please wait...\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::convolve_axis0_gpu_kernel$241(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","          1                            gld_efficiency             Global Memory Load Efficiency      12.50%      12.50%      12.50%\n","          1                            gst_efficiency            Global Memory Store Efficiency      12.50%      12.50%      12.50%\n","    Kernel: cudapy::__main__::convolve_axis1_gpu_kernel$242(Array<float, int=2, C, mutable, aligned>, Array<float, int=2, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","          1                            gld_efficiency             Global Memory Load Efficiency      70.05%      70.05%      70.05%\n","          1                            gst_efficiency            Global Memory Store Efficiency     100.00%     100.00%     100.00%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_2mBwUDf87yB"},"source":["# Exercise 3.2. Control flow: how code branching affects the performance."],"id":"_2mBwUDf87yB"},{"cell_type":"markdown","metadata":{"id":"5jF4RobZ2nDg"},"source":["Due to SIMT architecture of the CUDA multiprocessors, it is recommended to avoid different paths within the same warp.\n","\n","According to [CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html):\n","> Flow control instructions (if, switch, do, for, while) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp."],"id":"5jF4RobZ2nDg"},{"cell_type":"markdown","metadata":{"id":"nZNBa4eG2_Rj"},"source":["### Example"],"id":"nZNBa4eG2_Rj"},{"cell_type":"markdown","metadata":{"id":"4DwABDbg3BLR"},"source":["Let's implement the following function:\n","\n","```\n","y[i] = r[i]*a[i] + b[i]\n","```\n","\n","where `r[i] = i mod 8`.\n","\n","We can implement it in one of the two ways:\n","1. directly by definition (see `add_vectors_mod8_kernel`),\n","2. by doing a sequence of `if ... elif ... elif ... else` blocks (see `add_vectors_mod8_branches_kernel`)."],"id":"4DwABDbg3BLR"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gEV3cwOb3r8m","executionInfo":{"status":"ok","timestamp":1624691058292,"user_tz":-120,"elapsed":18,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"f1d6bbcc-eabd-48fb-a1ee-38c1468b6703"},"source":["%%writefile 3_2_control_flow.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","\n","block_size = 256\n","\n","\n","@cuda.jit\n","def add_vectors_mod8_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    r = i % 8 + 1\n","    result[i] = r*a[i] + b[i]\n","\n","\n","def add_vectors_mod8(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_mod8_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","@cuda.jit\n","def add_vectors_mod8_branches_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    if i % 8 == 0:\n","        result[i] = a[i] + b[i]\n","    elif i % 8 == 1:\n","        result[i] = 2*a[i] + b[i]\n","    elif i % 8 == 2:\n","        result[i] = 3*a[i] + b[i]\n","    elif i % 8 == 3:\n","        result[i] = 4*a[i] + b[i]\n","    elif i % 8 == 4:\n","        result[i] = 5*a[i] + b[i]\n","    elif i % 8 == 5:\n","        result[i] = 6*a[i] + b[i]\n","    elif i % 8 == 6:\n","        result[i] = 7*a[i] + b[i]\n","    elif i % 8 == 7:\n","        result[i] = 8*a[i] + b[i]\n","\n","\n","def add_vectors_mod8_branches(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_mod8_branches_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8)\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_mod8_branches)"],"id":"gEV3cwOb3r8m","execution_count":35,"outputs":[{"output_type":"stream","text":["Overwriting 3_2_control_flow.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yag_hEM8CEuc"},"source":["Let's check how much time does it take to execute each of the kernel:"],"id":"yag_hEM8CEuc"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3syxy_Dq4Piu","executionInfo":{"status":"ok","timestamp":1624691070082,"user_tz":-120,"elapsed":8172,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"a3f08ed5-b8d9-4dc0-ff21-013aee372740"},"source":["! nvprof --trace gpu python 3_2_control_flow.py"],"id":"3syxy_Dq4Piu","execution_count":36,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==69197== NVPROF is profiling process 69197, command: python 3_2_control_flow.py\n","Benchmark result: \n","Average processing time: 0.0178 seconds (+/- 0.0545), median: 0.0117\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0273 seconds (+/- 0.0253), median: 0.0239\n","==69197== Profiling application: python 3_2_control_flow.py\n","==69197== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   44.96%  1.33802s       100  13.380ms  1.4619ms  35.427ms  cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   31.22%  929.08ms       600  1.5485ms  1.2940ms  2.6280ms  [CUDA memcpy DtoH]\n","                   21.21%  631.13ms       400  1.5778ms  1.4059ms  2.6370ms  [CUDA memcpy HtoD]\n","                    2.60%  77.462ms       100  774.62us  304.19us  8.5429ms  cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a-Tf_qxCBm9h"},"source":["Let's also measure `branch_efficiency` metric defined as a:\n","> Ratio of non-divergent branches to total branches expressed as percentage."],"id":"a-Tf_qxCBm9h"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v7htXLWzAkEM","executionInfo":{"status":"ok","timestamp":1624691113554,"user_tz":-120,"elapsed":8832,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"7759ef5a-0dec-41ac-ca2c-aa401cf5bb2e"},"source":["! nvprof --metrics branch_efficiency python 3_2_control_flow.py"],"id":"v7htXLWzAkEM","execution_count":37,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==69273== NVPROF is profiling process 69273, command: python 3_2_control_flow.py\n","Benchmark result: \n","Average processing time: 0.0211 seconds (+/- 0.0468), median: 0.0151\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0305 seconds (+/- 0.0210), median: 0.0257\n","==69273== Profiling application: python 3_2_control_flow.py\n","==69273== Profiling result:\n","==69273== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_mod8_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                         branch_efficiency                         Branch Efficiency     100.00%     100.00%     100.00%\n","    Kernel: cudapy::__main__::add_vectors_mod8_branches_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                         branch_efficiency                         Branch Efficiency      58.82%      58.82%      58.82%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7rq1MbBJI0CO"},"source":["Of course, the above example has been artificially complicated just to show the effect of complex kernel logic on the kernel's performance."],"id":"7rq1MbBJI0CO"},{"cell_type":"markdown","metadata":{"id":"DaX4hxR09G0B"},"source":["# Exercise 3.3. Multiprocessor occupancy: thread block size."],"id":"DaX4hxR09G0B"},{"cell_type":"markdown","metadata":{"id":"pIGIxvK6JG5n"},"source":["Recall that:\n","> The number of threads per block should be a **multiple of 32 threads**, because this provides optimal computing efficiency and facilitates coalescing.\n","\n","[CUDA Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) also gives some other suggestions how to choose the proper number of threads per block:\n","\n","> There are many such factors involved in selecting block size, and inevitably some experimentation is required. However, a few rules of thumb should be followed:\n","> 1. Threads per block should be **a multiple of warp size** to avoid wasting computation on under-populated warps and to facilitate coalescing.\n","> 2. A **minimum of 64 threads** per block should be used, and only if there are multiple concurrent blocks per multiprocessor.\n","> 3. Between **128 and 256 threads** per block is a good initial range for experimentation with different block sizes.\n","> 4. Use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. This is particularly beneficial to kernels that frequently call __syncthreads()."],"id":"pIGIxvK6JG5n"},{"cell_type":"markdown","metadata":{"id":"VToK2NYJRRgk"},"source":["### Example"],"id":"VToK2NYJRRgk"},{"cell_type":"markdown","metadata":{"id":"xq06puf9Q2md"},"source":["Let's mesure `add_vectors`' occupancy for a different number of threads:\n"],"id":"xq06puf9Q2md"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oho6SbZORUFs","executionInfo":{"status":"ok","timestamp":1624694483181,"user_tz":-120,"elapsed":18,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"d8684a25-722b-47fb-a6a1-dfd983609b7f"},"source":["%%writefile 3_3_occupancy_16.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 16\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"oho6SbZORUFs","execution_count":39,"outputs":[{"output_type":"stream","text":["Writing 3_3_occupancy_16.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J58oLJpDRi9k","executionInfo":{"status":"ok","timestamp":1624694549583,"user_tz":-120,"elapsed":8908,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"1d9196a6-6e59-4deb-8641-49df2f9fd74d"},"source":["! nvprof --trace gpu python 3_3_occupancy_16.py"],"id":"J58oLJpDRi9k","execution_count":43,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==71869== NVPROF is profiling process 71869, command: python 3_3_occupancy_16.py\n","Benchmark result: \n","Average processing time: 0.0172 seconds (+/- 0.0486), median: 0.0120\n","==71869== Profiling application: python 3_3_occupancy_16.py\n","==71869== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   53.77%  455.71ms       300  1.5190ms  1.2940ms  2.6364ms  [CUDA memcpy DtoH]\n","                   38.61%  327.26ms       200  1.6363ms  1.4478ms  2.6613ms  [CUDA memcpy HtoD]\n","                    7.62%  64.599ms       100  645.99us  634.43us  794.43us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n","Benchmarking the function, please wait...\n","==71904== NVPROF is profiling process 71904, command: python 3_3_occupancy_16.py\n","Benchmark result: \n","Average processing time: 0.0238 seconds (+/- 0.0473), median: 0.0188\n","==71904== Profiling application: python 3_3_occupancy_16.py\n","==71904== Profiling result:\n","==71904== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                        achieved_occupancy                        Achieved Occupancy    0.303720    0.337505    0.323152\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LKyfKBGpWCtR"},"source":["According to NVIDIA documentation, `achieved_occupancy` measures:\n","> Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor."],"id":"LKyfKBGpWCtR"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"725eQmA6V5Fj","executionInfo":{"status":"ok","timestamp":1624694566933,"user_tz":-120,"elapsed":5190,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"c0f6a9db-3896-418d-e905-ea01ce26877e"},"source":["! nvprof --metrics achieved_occupancy python 3_3_occupancy_16.py"],"id":"725eQmA6V5Fj","execution_count":44,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==71957== NVPROF is profiling process 71957, command: python 3_3_occupancy_16.py\n","Benchmark result: \n","Average processing time: 0.0252 seconds (+/- 0.0450), median: 0.0189\n","==71957== Profiling application: python 3_3_occupancy_16.py\n","==71957== Profiling result:\n","==71957== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                        achieved_occupancy                        Achieved Occupancy    0.302625    0.338981    0.325882\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WBy_xzl6Rd0m","executionInfo":{"status":"ok","timestamp":1624694638495,"user_tz":-120,"elapsed":16,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"7ba64635-9ae7-4269-edeb-44f2017b8f42"},"source":["%%writefile 3_3_occupancy_256.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 256\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"WBy_xzl6Rd0m","execution_count":46,"outputs":[{"output_type":"stream","text":["Writing 3_3_occupancy_256.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYxXfEU8WOWU","executionInfo":{"status":"ok","timestamp":1624694671798,"user_tz":-120,"elapsed":8905,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"c38dcf5a-03e0-4d75-bf6b-87f973f896af"},"source":["! nvprof --trace gpu python 3_3_occupancy_256.py\n","! nvprof --metrics achieved_occupancy python 3_3_occupancy_256.py"],"id":"EYxXfEU8WOWU","execution_count":47,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==72133== NVPROF is profiling process 72133, command: python 3_3_occupancy_256.py\n","Benchmark result: \n","Average processing time: 0.0177 seconds (+/- 0.0553), median: 0.0118\n","==72133== Profiling application: python 3_3_occupancy_256.py\n","==72133== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   56.10%  456.25ms       300  1.5208ms  1.2900ms  2.6553ms  [CUDA memcpy DtoH]\n","                   38.58%  313.75ms       200  1.5687ms  1.3848ms  2.6183ms  [CUDA memcpy HtoD]\n","                    5.32%  43.290ms       100  432.90us  247.65us  921.06us  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n","Benchmarking the function, please wait...\n","==72168== NVPROF is profiling process 72168, command: python 3_3_occupancy_256.py\n","Benchmark result: \n","Average processing time: 0.0214 seconds (+/- 0.0481), median: 0.0162\n","==72168== Profiling application: python 3_3_occupancy_256.py\n","==72168== Profiling result:\n","==72168== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                        achieved_occupancy                        Achieved Occupancy    0.807554    0.899094    0.820068\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TFUXkREwReV8","executionInfo":{"status":"ok","timestamp":1624694722396,"user_tz":-120,"elapsed":18,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"92327c9b-7e0c-4e5a-ff84-0e1ef55acd6e"},"source":["%%writefile 3_3_occupancy_1024.py\n","\n","from numba import cuda\n","import math\n","import numpy as np\n","import gpu_short_course.tests\n","\n","block_size = 1024\n","\n","\n","@cuda.jit\n","def add_vectors_kernel(result, a, b):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(result):\n","        return\n","    result[i] = a[i] + b[i]\n","\n","\n","def add_vectors_gpu(a, b):\n","    result = cuda.device_array(shape=a.shape, dtype=a.dtype)\n","    grid_size = math.ceil(len(a)/block_size)\n","    add_vectors_kernel[grid_size, block_size](result, a, b)\n","    return result.copy_to_host()\n","\n","gpu_short_course.tests.benchmark_add_vectors(add_vectors_gpu)"],"id":"TFUXkREwReV8","execution_count":50,"outputs":[{"output_type":"stream","text":["Overwriting 3_3_occupancy_1024.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CNGVqycVWXyB","executionInfo":{"status":"ok","timestamp":1624694738602,"user_tz":-120,"elapsed":8820,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"50964494-cfe2-42c3-cb38-264a7a0a3c86"},"source":["! nvprof --trace gpu python 3_3_occupancy_1024.py\n","! nvprof --metrics achieved_occupancy python 3_3_occupancy_1024.py"],"id":"CNGVqycVWXyB","execution_count":51,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==72367== NVPROF is profiling process 72367, command: python 3_3_occupancy_1024.py\n","Benchmark result: \n","Average processing time: 0.0183 seconds (+/- 0.0486), median: 0.0132\n","==72367== Profiling application: python 3_3_occupancy_1024.py\n","==72367== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   56.29%  488.78ms       300  1.6293ms  1.2964ms  4.2568ms  [CUDA memcpy DtoH]\n","                   38.43%  333.72ms       200  1.6686ms  1.4115ms  3.5513ms  [CUDA memcpy HtoD]\n","                    5.28%  45.814ms       100  458.14us  249.70us  1.1106ms  cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","No API activities were profiled.\n","Benchmarking the function, please wait...\n","==72402== NVPROF is profiling process 72402, command: python 3_3_occupancy_1024.py\n","Benchmark result: \n","Average processing time: 0.0218 seconds (+/- 0.0496), median: 0.0165\n","==72402== Profiling application: python 3_3_occupancy_1024.py\n","==72402== Profiling result:\n","==72402== Metric result:\n","Invocations                               Metric Name                        Metric Description         Min         Max         Avg\n","Device \"GeForce MX250 (0)\"\n","    Kernel: cudapy::__main__::add_vectors_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","        100                        achieved_occupancy                        Achieved Occupancy    0.730620    0.842951    0.760431\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1dAsTyfuWmC0"},"source":["Finding the right number of threads per block requires some experimentation, but generally 128 or 256 threads is a good starting point."],"id":"1dAsTyfuWmC0"},{"cell_type":"markdown","metadata":{"id":"Ufa_O1_J9vTI"},"source":["# Exercise 3.4. Instruction-level optimizations."],"id":"Ufa_O1_J9vTI"},{"cell_type":"markdown","metadata":{"id":"GX352UzFpPsX"},"source":["This chapter covers the following:\n","\n","1. Impact of the data type selection and conversion on the kernel's performance.\n","2. Floating-point intrinsics."],"id":"GX352UzFpPsX"},{"cell_type":"markdown","metadata":{"id":"Cn_-MQH1Vn4l"},"source":["## Exercise 3.4.1. Data types."],"id":"Cn_-MQH1Vn4l"},{"cell_type":"markdown","metadata":{"id":"DUaJzNKup3SX"},"source":["When implementing a CUDA GPU kernel, keep the following in mind:\n","\n","- the choice of the input data type may affect the kernel performance,\n","- data type conversion in kernel implementation may affect kernel performance."],"id":"DUaJzNKup3SX"},{"cell_type":"markdown","metadata":{"id":"oRKBDy19qKZI"},"source":["Let's do some comparison of  `float32` and `float64` data, based on an example of a one-dimensional convolution.\n","\n","First, let's recall our baseline implementation of convolution operator.\n","\n","NOTE:\n","- our benchmark function generates `float32` input data,\n","- we used in the kernel implementation the `float32` keyword to enforce the proper data type of `value` variable. "],"id":"oRKBDy19qKZI"},{"cell_type":"code","metadata":{"id":"2Mq2y1_fW48S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624700759094,"user_tz":-120,"elapsed":19,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"645dd1a1-ad64-4913-a3dc-9e0faf60a98e"},"source":["%%writefile 3_4_1_convolve_float32.py\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","from gpu_short_course.tests import benchmark_convolve\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","    M, N = len(x), len(h)\n","    \n","    o = int(math.ceil(N/2)-1)\n","\n","    value = float32(0.0)\n","    for j in range(N):\n","        k = i+o-j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","    \n","\n","def convolve_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","benchmark_convolve(convolve_gpu, dtype=np.float32)"],"id":"2Mq2y1_fW48S","execution_count":1,"outputs":[{"output_type":"stream","text":["Overwriting 3_4_1_convolve_float32.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6iwhZPo4W8OM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624700781570,"user_tz":-120,"elapsed":5147,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"be2d7a28-49c9-4fe3-c4d8-648d27abc8ba"},"source":["! nvprof --trace gpu python 3_4_1_convolve_float32.py"],"id":"6iwhZPo4W8OM","execution_count":3,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==78701== NVPROF is profiling process 78701, command: python 3_4_1_convolve_float32.py\n","Benchmark result: \n","Average processing time: 0.0362 seconds (+/- 0.0585), median: 0.0302\n","==78701== Profiling application: python 3_4_1_convolve_float32.py\n","==78701== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   82.47%  2.26756s       100  22.676ms  17.149ms  30.855ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   11.61%  319.13ms       300  1.0638ms  1.1520us  2.6456ms  [CUDA memcpy DtoH]\n","                    5.92%  162.88ms       200  814.39us     928ns  2.5892ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kj_TCkJVsHfP"},"source":["Now let's get rid of the `float32` keyword on line `17` and see if it has any effect on performance."],"id":"kj_TCkJVsHfP"},{"cell_type":"code","metadata":{"id":"Wp7G2hWfW-5y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624700702244,"user_tz":-120,"elapsed":17,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"5b9ddd95-7ab6-424e-a3ee-908bf450cb35"},"source":["%%writefile 3_4_1_convolve_float32_and_float64.py\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","from gpu_short_course.tests import benchmark_convolve\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","    M, N = len(x), len(h)\n","    \n","    o = int(math.ceil(N/2)-1)\n","\n","    value = 0.0\n","    for j in range(N):\n","        k = i+o-j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","    \n","\n","def convolve_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","benchmark_convolve(convolve_gpu, dtype=np.float32)"],"id":"Wp7G2hWfW-5y","execution_count":75,"outputs":[{"output_type":"stream","text":["Writing 3_4_1_convolve_float32_and_float64.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3_ezLMZXASn","executionInfo":{"status":"ok","timestamp":1624700794518,"user_tz":-120,"elapsed":6671,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"7e94cda7-bdb7-42a7-f35b-e5ba11b35c10"},"source":["! nvprof --trace gpu python 3_4_1_convolve_float32_and_float64.py"],"id":"q3_ezLMZXASn","execution_count":4,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==78730== NVPROF is profiling process 78730, command: python 3_4_1_convolve_float32_and_float64.py\n","Benchmark result: \n","Average processing time: 0.0519 seconds (+/- 0.0569), median: 0.0473\n","==78730== Profiling application: python 3_4_1_convolve_float32_and_float64.py\n","==78730== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   89.74%  3.89248s       100  38.925ms  26.496ms  50.494ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                    6.74%  292.19ms       300  973.97us  1.1200us  2.6160ms  [CUDA memcpy DtoH]\n","                    3.52%  152.71ms       200  763.56us     960ns  2.5823ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z6c7gjfpXC9v"},"source":["The processing may increase, because 0.0 is a float64, and we are doing promotion from float32 to float64, i.e.:\n","\n","`value += (float64)(x[k]*h[j])`\n","\n","then we downgrade from float64 to float32:\n","\n","`y[i] = (float32)value`.\n","\n","(note: the above may vary between different GPUs)"],"id":"Z6c7gjfpXC9v"},{"cell_type":"markdown","metadata":{"id":"sUntEbVrXG6W"},"source":["Lets check what results we will get when we will use only `float64` values in computations."],"id":"sUntEbVrXG6W"},{"cell_type":"code","metadata":{"id":"N7jeCX3vXIk3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624701000999,"user_tz":-120,"elapsed":18,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"0aa80d36-0486-464c-8491-6ac116bb3dec"},"source":["%%writefile 3_4_1_convolve_float64.py\n","\n","import math\n","import numpy as np\n","from numba import cuda, float32\n","from gpu_short_course.tests import benchmark_convolve\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","    M, N = len(x), len(h)\n","    \n","    o = int(math.ceil(N/2)-1)\n","\n","    value = 0.0\n","    for j in range(N):\n","        k = i+o-j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","    \n","\n","def convolve_gpu(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","benchmark_convolve(convolve_gpu, dtype=np.float64)"],"id":"N7jeCX3vXIk3","execution_count":6,"outputs":[{"output_type":"stream","text":["Overwriting 3_4_1_convolve_float64.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UnnVJp5JXKHg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624701008183,"user_tz":-120,"elapsed":5372,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"cd6e80e9-d693-4078-be48-79bdd33219f4"},"source":["! nvprof --trace gpu python 3_4_1_convolve_float64.py"],"id":"UnnVJp5JXKHg","execution_count":7,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==79096== NVPROF is profiling process 79096, command: python 3_4_1_convolve_float64.py\n","Benchmark result: \n","Average processing time: 0.0373 seconds (+/- 0.0595), median: 0.0310\n","==79096== Profiling application: python 3_4_1_convolve_float64.py\n","==79096== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   66.93%  1.90606s       100  19.061ms  17.431ms  22.869ms  cudapy::__main__::convolve_gpu_kernel$241(Array<double, int=1, C, mutable, aligned>, Array<double, int=1, C, mutable, aligned>, Array<double, int=1, C, mutable, aligned>)\n","                   21.80%  620.79ms       300  2.0693ms  1.1520us  5.1669ms  [CUDA memcpy DtoH]\n","                   11.28%  321.16ms       200  1.6058ms     992ns  5.2513ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"484XwMSnugNk"},"source":["The above results may differ between different GPU cards."],"id":"484XwMSnugNk"},{"cell_type":"markdown","metadata":{"id":"eRPoAi2G9yay"},"source":["## Exercise 3.4.2. Floating point intrisincs."],"id":"eRPoAi2G9yay"},{"cell_type":"markdown","metadata":{"id":"A4kVCX0lurgR"},"source":["CUDA Math API provides a set of intrinsic functions, specialized to carry out various calculations. Sometimes, using an intrisinc function explicitly in your code may improve its performance.\n","\n","A complete list of intrinsic functions for CUDA C/C++ is available [here](https://docs.nvidia.com/cuda/cuda-math-api/).\n","\n","A complete list of floating-point intrinsics in Numba is avalable [here](https://numba.pydata.org/numba-doc/latest/cuda-reference/kernel.html#floating-point-intrinsics). \n","\n","Note: At the stage of optimizing the machine code, the compiler may decide to use the intrinsic function (the same or similar), regardless of whether we used it in our implementation or not. Still, if you want to be sure that the intrinsic function is used, we should call it explicitly."],"id":"A4kVCX0lurgR"},{"cell_type":"markdown","metadata":{"id":"kl2i6H0cwI41"},"source":["Let's check if using `cuda.fma` intrinsic in our `convolution` gives us any improvement:"],"id":"kl2i6H0cwI41"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Z5QSCr8XopT","executionInfo":{"status":"ok","timestamp":1624701701797,"user_tz":-120,"elapsed":28,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"ce0b5b23-b4fb-419e-c205-c5da56a5d301"},"source":["%%writefile 3_4_2_convolve_intrinsics.py\n","import math\n","from numba import cuda, float32\n","import numpy as np\n","import gpu_short_course.tests\n","import cupy as cp\n","\n","\n","@cuda.jit\n","def convolve_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","\n","    M, N = len(x), len(h)\n","    o = int(math.ceil(N/2)-1)\n","    \n","    value = float32(0.0)\n","    for j in range(N):\n","        k = i + o - j\n","        if k >= 0 and k < M:\n","            value += x[k]*h[j]\n","    y[i] = value\n","\n","\n","def convolve(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","@cuda.jit\n","def convolve_fma_gpu_kernel(y, x, h):\n","    i = cuda.blockIdx.x*cuda.blockDim.x + cuda.threadIdx.x\n","    if i >= len(y):\n","        return\n","\n","    M, N = len(x), len(h)\n","    o = int(math.ceil(N/2)-1)\n","    \n","    value = float32(0.0)\n","    for j in range(N):\n","        k = i + o - j\n","        if k >= 0 and k < M:\n","            value = cuda.fma(x[k], h[j], value)\n","    y[i] = value\n","\n","\n","def convolve_fma(x, h):\n","    y = cuda.device_array(x.shape, dtype=x.dtype)\n","    block_size = 256\n","    grid_size = math.ceil(len(y)/block_size)\n","    convolve_fma_gpu_kernel[grid_size, block_size](y, x, h)\n","    return y.copy_to_host()\n","\n","\n","gpu_short_course.tests.benchmark_convolve(convolve_fma)\n","gpu_short_course.tests.benchmark_convolve(convolve)"],"id":"0Z5QSCr8XopT","execution_count":8,"outputs":[{"output_type":"stream","text":["Writing 3_4_2_convolve_intrinsics.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMjAyrbSY9gP","executionInfo":{"status":"ok","timestamp":1624701713204,"user_tz":-120,"elapsed":8427,"user":{"displayName":"Piotr Jarosik","photoUrl":"","userId":"07939774369485222074"}},"outputId":"6cc62eb4-3d7c-46c2-e1bd-6c2fae9831c6"},"source":["!nvprof --trace gpu python 3_4_1_convolve_gpu.py"],"id":"JMjAyrbSY9gP","execution_count":9,"outputs":[{"output_type":"stream","text":["Benchmarking the function, please wait...\n","==80097== NVPROF is profiling process 80097, command: python 3_4_1_convolve_gpu.py\n","Benchmark result: \n","Average processing time: 0.0324 seconds (+/- 0.0586), median: 0.0257\n","Benchmarking the function, please wait...\n","Benchmark result: \n","Average processing time: 0.0280 seconds (+/- 0.0099), median: 0.0263\n","==80097== Profiling application: python 3_4_1_convolve_gpu.py\n","==80097== Profiling result:\n","            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n"," GPU activities:   40.44%  1.94581s       100  19.458ms  17.212ms  28.853ms  cudapy::__main__::convolve_gpu_kernel$242(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   39.87%  1.91840s       100  19.184ms  17.029ms  27.130ms  cudapy::__main__::convolve_gpu_kernel_fma$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n","                   13.02%  626.64ms       600  1.0444ms     416ns  2.8632ms  [CUDA memcpy DtoH]\n","                    6.67%  320.89ms       400  802.21us     896ns  3.1757ms  [CUDA memcpy HtoD]\n","No API activities were profiled.\n"],"name":"stdout"}]}]}