{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dietary-princeton",
   "metadata": {},
   "source": [
    "# 2.2. Memory model: shared memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "taken-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import cuda, float32, int32\n",
    "import cupy as cp\n",
    "from tests import test_convolve, benchmark_convolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-jewel",
   "metadata": {},
   "source": [
    "## How much shared memory do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-cancellation",
   "metadata": {},
   "source": [
    "Maximum shared memory size per thread block (bytes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-seeking",
   "metadata": {},
   "source": [
    "Note: cupy seems to provide much more information about the device attributes, than Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neutral-observation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: b'GeForce MX250'\n",
      "Shared memory per thread block: 49152 [bytes]\n"
     ]
    }
   ],
   "source": [
    "device_props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "\n",
    "print(f\"Device name: {device_props['name']}\")\n",
    "print(f\"Shared memory per thread block: {device_props['sharedMemPerBlock']} [bytes]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-cornwall",
   "metadata": {},
   "source": [
    "A complete description of the device properties is available here:\n",
    "https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-capability",
   "metadata": {},
   "source": [
    "The below thread block size and shared memory size is enough to be run on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amended-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_BLOCK_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-frontier",
   "metadata": {},
   "source": [
    "## How to use shared memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "floppy-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x, h):\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    if i >= y.shape[0]:\n",
    "        return\n",
    "    \n",
    "    M = len(x)\n",
    "    N = len(h)\n",
    "    \n",
    "    x_shared = cuda.shared.array(shape=0, dtype=float32)\n",
    "    SHARED_SIZE = cuda.blockDim.x+N-1\n",
    "    OFFSET = int32(math.ceil(N/2)-1)\n",
    "    \n",
    "    # Copy a portion of data from global memory to shared memory.\n",
    "    \n",
    "    # The current position in the global memory.\n",
    "    k = i-(N-1)+OFFSET \n",
    "    # The current position in the shared memory.\n",
    "    k_shared = cuda.threadIdx.x \n",
    "    while k_shared < SHARED_SIZE:\n",
    "        if k >= 0 and k < M:\n",
    "            x_shared[k_shared] = x[k]\n",
    "        else:\n",
    "            x_shared[k_shared] = float32(0.0)\n",
    "        k_shared += cuda.blockDim.x\n",
    "        k        += cuda.blockDim.x\n",
    "\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    k_shared = cuda.threadIdx.x+N-1\n",
    "    value = float32(0.0)\n",
    "    for j in range(N):\n",
    "        value += x_shared[k_shared-j]*h[j]\n",
    "        \n",
    "    y[i] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pressing-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_gpu(y, x, h):\n",
    "    if y is None:\n",
    "        y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    \n",
    "    # Determine thread and block size.\n",
    "    n_threads = min(THREAD_BLOCK_SIZE, len(y))\n",
    "    block_size = (n_threads, )\n",
    "    grid_size = (math.ceil(len(y)/block_size[0]), )\n",
    "    \n",
    "    # Determine shared memory size.\n",
    "    N = len(h)\n",
    "    SHARED_SIZE = THREAD_BLOCK_SIZE+N-1\n",
    "    SHARED_SIZE_BYTES = SHARED_SIZE*y.dtype.itemsize\n",
    "    \n",
    "    if SHARED_SIZE_BYTES > device_props['sharedMemPerBlock']:    \n",
    "        raise ValueError(\"Declared shared memory size exceeds the amount available for the device.\")\n",
    "    \n",
    "    # Execute the kernel.\n",
    "    convolve_gpu_kernel[grid_size, block_size, cuda.default_stream(), SHARED_SIZE_BYTES](y, x, h)\n",
    "    return y.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "clean-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "test_convolve(lambda x, h: convolve_gpu(None, x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-outdoors",
   "metadata": {},
   "source": [
    "## How much improvement using shared memory gives us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfdf9cb-b95b-4fea-9081-eb41709d964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2_2_convolve_shared_memory.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2_2_convolve_shared_memory.py\n",
    "\n",
    "import math\n",
    "from numba import cuda, float32, int32\n",
    "import cupy as cp\n",
    "from tests import test_convolve, benchmark_convolve\n",
    "\n",
    "THREAD_BLOCK_SIZE = 256\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x, h):\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    if i >= y.shape[0]:\n",
    "        return\n",
    "    \n",
    "    M = len(x)\n",
    "    N = len(h)\n",
    "    \n",
    "    x_shared = cuda.shared.array(shape=0, dtype=float32)\n",
    "    SHARED_SIZE = cuda.blockDim.x+N-1\n",
    "    OFFSET = int32(math.ceil(N/2)-1)\n",
    "    \n",
    "    # Copy a portion of data from global memory to shared memory.\n",
    "    \n",
    "    # The current position in the global memory.\n",
    "    k = i-(N-1)+OFFSET \n",
    "    # The current position in the shared memory.\n",
    "    k_shared = cuda.threadIdx.x \n",
    "    while k_shared < SHARED_SIZE:\n",
    "        if k >= 0 and k < M:\n",
    "            x_shared[k_shared] = x[k]\n",
    "        else:\n",
    "            x_shared[k_shared] = float32(0.0)\n",
    "        k_shared += cuda.blockDim.x\n",
    "        k        += cuda.blockDim.x\n",
    "\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    k_shared = cuda.threadIdx.x+N-1\n",
    "    value = float32(0.0)\n",
    "    for j in range(N):\n",
    "        value += x_shared[k_shared-j]*h[j]\n",
    "        \n",
    "    y[i] = value\n",
    "    \n",
    "def convolve_gpu(y, x, h):\n",
    "    if y is None:\n",
    "        y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    \n",
    "    # Determine thread and block size.\n",
    "    n_threads = min(THREAD_BLOCK_SIZE, len(y))\n",
    "    block_size = (n_threads, )\n",
    "    grid_size = (math.ceil(len(y)/block_size[0]), )\n",
    "    \n",
    "    # Determine shared memory size.\n",
    "    N = len(h)\n",
    "    SHARED_SIZE = THREAD_BLOCK_SIZE+N-1\n",
    "    SHARED_SIZE_BYTES = SHARED_SIZE*y.dtype.itemsize\n",
    "    \n",
    "    # Execute the kernel.\n",
    "    convolve_gpu_kernel[grid_size, block_size, cuda.default_stream(), SHARED_SIZE_BYTES](y, x, h)\n",
    "    return y.copy_to_host()\n",
    "\n",
    "\n",
    "benchmark_convolve(lambda x, h: convolve_gpu(None, x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4693f39-0d9f-4f5a-9dca-0c65c74bc013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==25307== NVPROF is profiling process 25307, command: python 2_2_convolve_shared_memory.py\n",
      "Benchmark result: \n",
      "Average processing time: 0.0255 seconds (+/- 0.0601), median: 0.0192\n",
      "==25307== Profiling application: python 2_2_convolve_shared_memory.py\n",
      "==25307== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   76.80%  1.36432s       100  13.643ms  13.227ms  15.811ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                   14.84%  263.65ms       300  878.84us  1.1200us  2.5858ms  [CUDA memcpy DtoH]\n",
      "                    8.36%  148.52ms       200  742.58us     896ns  1.5442ms  [CUDA memcpy HtoD]\n",
      "      API calls:   77.61%  1.53060s       200  7.6530ms  12.728us  17.509ms  cuMemcpyDtoHAsync\n",
      "                    7.45%  146.87ms       100  1.4687ms  1.4370ms  2.7823ms  cuMemcpyDtoH\n",
      "                    5.98%  118.00ms         1  118.00ms  118.00ms  118.00ms  cuDevicePrimaryCtxRetain\n",
      "                    5.44%  107.36ms       200  536.81us  6.2540us  1.1265ms  cuMemcpyHtoDAsync\n",
      "                    1.99%  39.220ms       300  130.73us  3.4550us  6.8267ms  cuMemAlloc\n",
      "                    1.39%  27.503ms       297  92.603us  2.2720us  149.36us  cuMemFree\n",
      "                    0.07%  1.3570ms       100  13.570us  9.3800us  30.384us  cuLaunchKernel\n",
      "                    0.02%  421.33us      1103     381ns     190ns  8.5860us  cuCtxGetCurrent\n",
      "                    0.01%  279.93us      1101     254ns     139ns  1.4720us  cuCtxGetDevice\n",
      "                    0.01%  144.08us         1  144.08us  144.08us  144.08us  cuModuleLoadDataEx\n",
      "                    0.01%  113.83us       101  1.1260us      98ns  49.374us  cuDeviceGetAttribute\n",
      "                    0.00%  96.847us         1  96.847us  96.847us  96.847us  cuLinkComplete\n",
      "                    0.00%  92.928us         1  92.928us  92.928us  92.928us  cuLinkAddData\n",
      "                    0.00%  70.392us         2  35.196us  34.395us  35.997us  cuDeviceGetName\n",
      "                    0.00%  43.499us         1  43.499us  43.499us  43.499us  cuLinkCreate\n",
      "                    0.00%  43.239us         1  43.239us  43.239us  43.239us  cuDeviceTotalMem\n",
      "                    0.00%  20.949us         1  20.949us  20.949us  20.949us  cuMemGetInfo\n",
      "                    0.00%  4.1150us         1  4.1150us  4.1150us  4.1150us  cuDeviceGetPCIBusId\n",
      "                    0.00%  3.1190us         4     779ns     327ns  1.4310us  cuDeviceGetCount\n",
      "                    0.00%  2.4260us         3     808ns     580ns  1.0500us  cuDeviceGet\n",
      "                    0.00%  2.0950us         1  2.0950us  2.0950us  2.0950us  cuInit\n",
      "                    0.00%  1.4840us         5     296ns     135ns     687ns  cuFuncGetAttribute\n",
      "                    0.00%  1.3740us         1  1.3740us  1.3740us  1.3740us  cuDriverGetVersion\n",
      "                    0.00%  1.2850us         1  1.2850us  1.2850us  1.2850us  cuLinkDestroy\n",
      "                    0.00%  1.1590us         1  1.1590us  1.1590us  1.1590us  cuCtxPushCurrent\n",
      "                    0.00%  1.1410us         1  1.1410us  1.1410us  1.1410us  cuDeviceComputeCapability\n",
      "                    0.00%  1.0980us         1  1.0980us  1.0980us  1.0980us  cuModuleGetFunction\n",
      "                    0.00%     928ns         1     928ns     928ns     928ns  cudaRuntimeGetVersion\n",
      "                    0.00%     257ns         1     257ns     257ns     257ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "! nvprof python 2_2_convolve_shared_memory.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
