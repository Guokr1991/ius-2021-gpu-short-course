{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "equivalent-final",
   "metadata": {},
   "source": [
    "# 2.3 Memory model: constant memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "motivated-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import cuda, float32, int32\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from tests import test_convolve_const"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-designation",
   "metadata": {},
   "source": [
    "## How much constant memory we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "united-friend",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: b'GeForce MX250'\n",
      "Shared memory per thread block: 65536 [bytes]\n"
     ]
    }
   ],
   "source": [
    "device_props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "\n",
    "print(f\"Device name: {device_props['name']}\")\n",
    "print(f\"Shared memory per thread block: {device_props['totalConstMem']} [bytes]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-romania",
   "metadata": {},
   "source": [
    "## How to use constant memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "religious-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "THREAD_BLOCK_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-drill",
   "metadata": {},
   "source": [
    "- if we assume that filter coefficients doesn't change in the runtime, we can put them into the constant memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continued-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_host_const = np.random.rand(5).astype(np.float32)\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x):\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    if i >= y.shape[0]:\n",
    "        return\n",
    "    \n",
    "    # Constant memory\n",
    "    h_gpu_const = cuda.const.array_like(h_host_const)\n",
    "    \n",
    "    M = len(x)\n",
    "    N = len(h_gpu_const)\n",
    "    OFFSET = int32(math.ceil(N/2)-1)\n",
    "    \n",
    "    \n",
    "    # Shared memory\n",
    "    x_shared = cuda.shared.array(shape=0, dtype=float32)\n",
    "    SHARED_SIZE = cuda.blockDim.x+N-1\n",
    "    \n",
    "    \n",
    "    # Copy a portion of data from global memory to shared memory.\n",
    "    # The current position in the global memory.\n",
    "    k = i-(N-1)+OFFSET \n",
    "    # The current position in the shared memory.\n",
    "    k_shared = cuda.threadIdx.x \n",
    "    while k_shared < SHARED_SIZE:\n",
    "        if k >= 0 and k < M:\n",
    "            x_shared[k_shared] = x[k]\n",
    "        else:\n",
    "            x_shared[k_shared] = float32(0.0)\n",
    "        k_shared += cuda.blockDim.x\n",
    "        k        += cuda.blockDim.x\n",
    "\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    k_shared = cuda.threadIdx.x+N-1\n",
    "    value = float32(0.0)\n",
    "    for j in range(N):\n",
    "        value += x_shared[k_shared-j]*h_gpu_const[j]\n",
    "        \n",
    "    y[i] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "olympic-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve_gpu(y, x):\n",
    "    if y is None:\n",
    "        y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    \n",
    "    # Determine thread and block size.\n",
    "    n_threads = min(THREAD_BLOCK_SIZE, len(y))\n",
    "    block_size = (n_threads, )\n",
    "    grid_size = (math.ceil(len(y)/block_size[0]), )\n",
    "    \n",
    "    # Determine shared memory size.\n",
    "    N = len(h_host_const)\n",
    "    SHARED_SIZE = THREAD_BLOCK_SIZE+N-1\n",
    "    SHARED_SIZE_BYTES = SHARED_SIZE*y.dtype.itemsize\n",
    "    \n",
    "    if SHARED_SIZE_BYTES > device_props['sharedMemPerBlock']:    \n",
    "        raise ValueError(\"Declared shared memory size exceeds the amount available for the device.\")\n",
    "    \n",
    "    # Execute the kernel.\n",
    "    convolve_gpu_kernel[grid_size, block_size, cuda.default_stream(), SHARED_SIZE_BYTES](y, x)\n",
    "    return y.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "essential-diary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed.\n"
     ]
    }
   ],
   "source": [
    "test_convolve_const(lambda x: convolve_gpu(None, x), h_host_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88f2957-4512-49ef-8824-f2ec54be3af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 2_3_convolve_const_memory.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile 2_3_convolve_const_memory.py\n",
    "\n",
    "\n",
    "import math\n",
    "from numba import cuda, float32, int32\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from tests import benchmark_convolve_const, DEFAULT_BENCHMARK_H_SIZE\n",
    "\n",
    "\n",
    "THREAD_BLOCK_SIZE = 256\n",
    "\n",
    "\n",
    "h_host_const = np.random.rand(DEFAULT_BENCHMARK_H_SIZE).astype(np.float32)\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def convolve_gpu_kernel(y, x):\n",
    "    i = cuda.grid(1)\n",
    "    \n",
    "    if i >= y.shape[0]:\n",
    "        return\n",
    "    \n",
    "    # Constant memory\n",
    "    h_gpu_const = cuda.const.array_like(h_host_const)\n",
    "    \n",
    "    M = len(x)\n",
    "    N = len(h_gpu_const)\n",
    "    OFFSET = int32(math.ceil(N/2)-1)\n",
    "    \n",
    "    \n",
    "    # Shared memory\n",
    "    x_shared = cuda.shared.array(shape=0, dtype=float32)\n",
    "    SHARED_SIZE = cuda.blockDim.x+N-1\n",
    "    \n",
    "    \n",
    "    # Copy a portion of data from global memory to shared memory.\n",
    "    # The current position in the global memory.\n",
    "    k = i-(N-1)+OFFSET \n",
    "    # The current position in the shared memory.\n",
    "    k_shared = cuda.threadIdx.x \n",
    "    while k_shared < SHARED_SIZE:\n",
    "        if k >= 0 and k < M:\n",
    "            x_shared[k_shared] = x[k]\n",
    "        else:\n",
    "            x_shared[k_shared] = float32(0.0)\n",
    "        k_shared += cuda.blockDim.x\n",
    "        k        += cuda.blockDim.x\n",
    "\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    k_shared = cuda.threadIdx.x+N-1\n",
    "    value = float32(0.0)\n",
    "    for j in range(N):\n",
    "        value += x_shared[k_shared-j]*h_gpu_const[j]\n",
    "        \n",
    "    y[i] = value\n",
    "\n",
    "    \n",
    "def convolve_gpu(y, x):\n",
    "    if y is None:\n",
    "        y = cuda.device_array(x.shape, dtype=x.dtype)\n",
    "    \n",
    "    # Determine thread and block size.\n",
    "    n_threads = min(THREAD_BLOCK_SIZE, len(y))\n",
    "    block_size = (n_threads, )\n",
    "    grid_size = (math.ceil(len(y)/block_size[0]), )\n",
    "    \n",
    "    # Determine shared memory size.\n",
    "    N = len(h_host_const)\n",
    "    SHARED_SIZE = THREAD_BLOCK_SIZE+N-1\n",
    "    SHARED_SIZE_BYTES = SHARED_SIZE*y.dtype.itemsize\n",
    "    \n",
    "    # Execute the kernel.\n",
    "    convolve_gpu_kernel[grid_size, block_size, cuda.default_stream(), SHARED_SIZE_BYTES](y, x)\n",
    "    return y.copy_to_host()    \n",
    "\n",
    "benchmark_convolve_const(lambda x: convolve_gpu(None, x), h_host_const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "boxed-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==28454== NVPROF is profiling process 28454, command: python 2_3_convolve_const_memory.py\n",
      "Benchmark result: \n",
      "Average processing time: 0.0229 seconds (+/- 0.0634), median: 0.0159\n",
      "==28454== Profiling application: python 2_3_convolve_const_memory.py\n",
      "==28454== Profiling result:\n",
      "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
      " GPU activities:   72.65%  1.08956s       100  10.896ms  10.618ms  13.389ms  cudapy::__main__::convolve_gpu_kernel$241(Array<float, int=1, C, mutable, aligned>, Array<float, int=1, C, mutable, aligned>)\n",
      "                   17.42%  261.21ms       200  1.3060ms  1.2894ms  2.1076ms  [CUDA memcpy DtoH]\n",
      "                    9.94%  149.02ms       100  1.4902ms  1.3706ms  1.5719ms  [CUDA memcpy HtoD]\n",
      "      API calls:   73.87%  1.28092s       100  12.809ms  12.462ms  15.842ms  cuMemcpyDtoHAsync\n",
      "                    8.42%  146.07ms       100  1.4607ms  1.4382ms  1.7524ms  cuMemcpyDtoH\n",
      "                    6.22%  107.95ms       100  1.0795ms  984.27us  1.4710ms  cuMemcpyHtoDAsync\n",
      "                    6.06%  105.11ms         1  105.11ms  105.11ms  105.11ms  cuDevicePrimaryCtxRetain\n",
      "                    2.30%  39.811ms         1  39.811ms  39.811ms  39.811ms  cuLinkAddData\n",
      "                    1.61%  27.879ms       198  140.80us  76.172us  1.3763ms  cuMemFree\n",
      "                    1.39%  24.029ms       200  120.14us  71.918us  2.7276ms  cuMemAlloc\n",
      "                    0.08%  1.3148ms       100  13.148us  9.7500us  28.890us  cuLaunchKernel\n",
      "                    0.02%  286.87us       803     357ns     191ns  1.5370us  cuCtxGetCurrent\n",
      "                    0.01%  208.86us       801     260ns     123ns  2.9450us  cuCtxGetDevice\n",
      "                    0.01%  145.70us         1  145.70us  145.70us  145.70us  cuModuleLoadDataEx\n",
      "                    0.01%  113.91us       101  1.1270us     101ns  48.884us  cuDeviceGetAttribute\n",
      "                    0.01%  90.147us         1  90.147us  90.147us  90.147us  cuLinkComplete\n",
      "                    0.00%  61.089us         2  30.544us  25.650us  35.439us  cuDeviceGetName\n",
      "                    0.00%  45.808us         1  45.808us  45.808us  45.808us  cuDeviceTotalMem\n",
      "                    0.00%  41.060us         1  41.060us  41.060us  41.060us  cuLinkCreate\n",
      "                    0.00%  21.975us         1  21.975us  21.975us  21.975us  cuMemGetInfo\n",
      "                    0.00%  4.5220us         1  4.5220us  4.5220us  4.5220us  cuDeviceGetPCIBusId\n",
      "                    0.00%  2.6330us         4     658ns     147ns  1.5480us  cuDeviceGetCount\n",
      "                    0.00%  1.7310us         1  1.7310us  1.7310us  1.7310us  cuModuleGetFunction\n",
      "                    0.00%  1.5620us         5     312ns     132ns     759ns  cuFuncGetAttribute\n",
      "                    0.00%  1.5540us         1  1.5540us  1.5540us  1.5540us  cuInit\n",
      "                    0.00%  1.4650us         3     488ns     237ns     722ns  cuDeviceGet\n",
      "                    0.00%  1.3080us         1  1.3080us  1.3080us  1.3080us  cuCtxPushCurrent\n",
      "                    0.00%  1.1390us         1  1.1390us  1.1390us  1.1390us  cuLinkDestroy\n",
      "                    0.00%     594ns         1     594ns     594ns     594ns  cuDriverGetVersion\n",
      "                    0.00%     447ns         1     447ns     447ns     447ns  cudaRuntimeGetVersion\n",
      "                    0.00%     311ns         1     311ns     311ns     311ns  cuDeviceComputeCapability\n",
      "                    0.00%     195ns         1     195ns     195ns     195ns  cuDeviceGetUuid\n"
     ]
    }
   ],
   "source": [
    "! nvprof python 2_3_convolve_const_memory.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
